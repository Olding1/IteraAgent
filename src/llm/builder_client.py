"""Builder API client for construction-time LLM calls."""

from typing import Optional, Type, Any, TypeVar
from pydantic import BaseModel, Field
import httpx
import os
import json

# Optional imports for different providers
try:
    from langchain_openai import ChatOpenAI
    HAS_OPENAI = True
except ImportError:
    HAS_OPENAI = False

try:
    from langchain_anthropic import ChatAnthropic
    HAS_ANTHROPIC = True
except ImportError:
    HAS_ANTHROPIC = False

from src.utils.json_utils import extract_json_from_text

T = TypeVar("T", bound=BaseModel)


class BuilderAPIConfig(BaseModel):
    """Configuration for Builder API."""

    provider: str = Field(..., description="API provider (openai/anthropic/azure)")
    model: str = Field(..., description="Model name")
    api_key: str = Field(..., description="API key")
    base_url: Optional[str] = Field(default=None, description="Custom base URL")
    timeout: int = Field(default=60, description="Timeout in seconds")
    max_retries: int = Field(default=3, description="Maximum retry attempts")
    temperature: float = Field(default=0.7, ge=0.0, le=2.0, description="Temperature")


class BuilderClient:
    """Builder API client for construction-time LLM calls.
    
    This client is used by PM, Graph Designer, RAG Builder, and other
    construction-time components. It uses a powerful model (GPT-4o, Claude 3.5)
    to generate high-quality agent designs.
    """

    def __init__(self, config: BuilderAPIConfig):
        """Initialize Builder API client.

        Args:
            config: Builder API configuration
        """
        self.config = config
        self.client = self._init_client(config)

        # ðŸ†• Phase 5: Token ç»Ÿè®¡
        self.token_stats = {
            "total_calls": 0,
            "total_input_tokens": 0,
            "total_output_tokens": 0,
            "total_cost_usd": 0.0
        }

    def _init_client(self, config: BuilderAPIConfig) -> Any:
        """Initialize LLM client based on provider.
        
        Args:
            config: Builder API configuration
            
        Returns:
            Initialized LLM client
        """
        if config.provider == "openai":
            if not HAS_OPENAI:
                raise ImportError(
                    "langchain-openai is not installed. "
                    "Install it with: pip install langchain-openai"
                )
            return ChatOpenAI(
                model=config.model,
                api_key=config.api_key,
                base_url=config.base_url,
                temperature=config.temperature,
                timeout=config.timeout,
                max_retries=config.max_retries,
            )
        elif config.provider == "anthropic":
            if not HAS_ANTHROPIC:
                raise ImportError(
                    "langchain-anthropic is not installed. "
                    "Install it with: pip install langchain-anthropic"
                )
            return ChatAnthropic(
                model=config.model,
                api_key=config.api_key,
                temperature=config.temperature,
                timeout=config.timeout,
                max_retries=config.max_retries,
            )
        else:
            raise ValueError(f"Unsupported provider: {config.provider}")

    async def call(
        self, prompt: str, schema: Optional[Type[BaseModel]] = None
    ) -> str | BaseModel:
        """Call Builder API with optional structured output.

        Args:
            prompt: Input prompt
            schema: Optional Pydantic schema for structured output

        Returns:
            Response string or structured output
        """
        if schema:
            # Use new universal structured generator
            return await self.generate_structured(prompt, schema)
        else:
            # Regular text output
            response = await self.client.ainvoke(prompt)
            # ðŸ†• Phase 5: ç»Ÿè®¡ Token
            self._update_token_stats(response)
            return response.content

    async def generate_structured(
        self, 
        prompt: str, 
        response_model: Type[T],
        temperature: Optional[float] = None
    ) -> T:
        """
        é€šç”¨çš„ç»“æž„åŒ–è¾“å‡ºç”Ÿæˆå™¨
        è‡ªåŠ¨å¤„ç† DeepSeek ç­‰ä¸æ”¯æŒ response_format çš„æƒ…å†µ
        
        Args:
            prompt: è¾“å…¥æç¤ºè¯
            response_model: Pydantic æ¨¡åž‹ç±»
            temperature: å¯é€‰çš„æ¸©åº¦å‚æ•°
        
        Returns:
            éªŒè¯åŽçš„ Pydantic æ¨¡åž‹å®žä¾‹
        """
        temp = temperature if temperature is not None else self.config.temperature
        
        # èŽ·å– Pydantic çš„ Schema
        schema = response_model.model_json_schema()
        schema_str = json.dumps(schema, indent=2, ensure_ascii=False)

        # -------------------------------------------------------
        # å°è¯• 1: åŽŸç”Ÿæ”¯æŒæ¨¡å¼ (LangChain with_structured_output)
        # -------------------------------------------------------
        try:
            structured_llm = self.client.with_structured_output(response_model)
            result = await structured_llm.ainvoke(prompt)
            # ðŸ†• Phase 5: ç»Ÿè®¡ Token (å°è¯•ä»Ž result ä¸­æå–)
            if hasattr(result, '__dict__'):
                # å¦‚æžœ result æ˜¯å¯¹è±¡ï¼Œå°è¯•èŽ·å–åŽŸå§‹å“åº”
                pass  # structured output é€šå¸¸ä¸åŒ…å« usage ä¿¡æ¯
            return result

        except Exception as e:
            # æ•èŽ·å„ç§å¯èƒ½çš„é”™è¯¯
            error_str = str(e).lower()
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ response_format ä¸æ”¯æŒçš„é”™è¯¯
            if any(keyword in error_str for keyword in [
                "response_format", "unavailable", "400", 
                "bad request", "invalid_request_error"
            ]):
                print(f"âš ï¸  API ä¸æ”¯æŒåŽŸç”Ÿ JSON æ¨¡å¼ï¼Œåˆ‡æ¢åˆ° Prompt å¢žå¼ºæ¨¡å¼...")
                return await self._generate_structured_fallback(
                    prompt, response_model, schema_str, temp
                )
            else:
                # å…¶ä»–é”™è¯¯ï¼ˆå¦‚ä½™é¢ä¸è¶³ï¼‰ç›´æŽ¥æŠ›å‡º
                raise e

    async def _generate_structured_fallback(
        self, 
        prompt: str, 
        response_model: Type[T], 
        schema_str: str,
        temperature: float
    ) -> T:
        """
        å›žé€€æ¨¡å¼ï¼šé€šè¿‡ Prompt å¼ºåˆ¶æ¨¡åž‹è¾“å‡º JSONï¼Œå¹¶ä½¿ç”¨æ­£åˆ™æå–
        
        Args:
            prompt: åŽŸå§‹æç¤ºè¯
            response_model: Pydantic æ¨¡åž‹ç±»
            schema_str: JSON Schema å­—ç¬¦ä¸²
            temperature: æ¸©åº¦å‚æ•°
        
        Returns:
            éªŒè¯åŽçš„ Pydantic æ¨¡åž‹å®žä¾‹
        """
        # 1. æ·±åº¦ä¿®æ”¹ Promptï¼šæŠŠ Schema å¡žè¿›åŽ»
        fallback_prompt = (
            f"{prompt}\n\n"
            f"ðŸ›‘ CRITICAL INSTRUCTION: OUTPUT FORMAT ENFORCEMENT ðŸ›‘\n"
            f"You MUST output a valid JSON object matching the following schema.\n"
            f"Do NOT include any conversational text, explanations, or markdown code blocks.\n"
            f"Output ONLY the raw JSON object.\n\n"
            f"Required JSON Schema:\n"
            f"```json\n{schema_str}\n```\n\n"
            f"Your response (JSON only):"
        )

        # 2. æ™®é€šæ–‡æœ¬æ¨¡å¼è°ƒç”¨
        response = await self.client.ainvoke(fallback_prompt)
        raw_text = response.content

        # ðŸ†• Phase 5: ç»Ÿè®¡ Token
        self._update_token_stats(response)

        # 3. æ¸…æ´—å’Œè§£æž
        try:
            json_str = extract_json_from_text(raw_text)
        except ValueError as e:
            print(f"âŒ JSON æå–å¤±è´¥: {e}")
            print(f"åŽŸå§‹æ–‡æœ¬: {raw_text[:200]}...")
            raise ValueError(f"Failed to extract JSON from LLM response: {e}")
        
        # 4. Pydantic æ ¡éªŒ (è¿™ä¸€æ­¥æœ€å…³é”®ï¼Œç¡®ä¿æ ¼å¼å¯¹äº†)
        try:
            return response_model.model_validate_json(json_str)
        except Exception as e:
            print(f"âŒ Pydantic éªŒè¯å¤±è´¥: {e}")
            print(f"æå–çš„ JSON: {json_str[:200]}...")
            raise ValueError(f"Failed to validate JSON against schema: {e}")

    async def health_check(self) -> bool:
        """Check API connectivity.

        Returns:
            True if API is accessible, False otherwise
        """
        try:
            # Simple test call
            response = await self.client.ainvoke("Hello")
            return True
        except Exception as e:
            print(f"Builder API health check failed: {e}")
            return False

    def _update_token_stats(self, response: Any):
        """
        æ›´æ–° Token ç»Ÿè®¡ä¿¡æ¯

        Args:
            response: LLM å“åº”å¯¹è±¡
        """
        # å°è¯•ä»Žå“åº”ä¸­æå– usage ä¿¡æ¯
        usage = None

        # LangChain å“åº”å¯¹è±¡é€šå¸¸æœ‰ response_metadata
        if hasattr(response, 'response_metadata'):
            usage = response.response_metadata.get('token_usage')

        # æˆ–è€…ç›´æŽ¥æœ‰ usage å±žæ€§
        if not usage and hasattr(response, 'usage'):
            usage = response.usage

        if usage:
            self.token_stats["total_calls"] += 1

            # æå– token æ•°é‡
            input_tokens = usage.get('prompt_tokens', 0)
            output_tokens = usage.get('completion_tokens', 0)

            self.token_stats["total_input_tokens"] += input_tokens
            self.token_stats["total_output_tokens"] += output_tokens

            # è®¡ç®—æˆæœ¬
            cost = self._calculate_cost(input_tokens, output_tokens)
            self.token_stats["total_cost_usd"] += cost

    def _calculate_cost(self, input_tokens: int, output_tokens: int) -> float:
        """
        è®¡ç®— API è°ƒç”¨æˆæœ¬

        Args:
            input_tokens: è¾“å…¥ token æ•°é‡
            output_tokens: è¾“å‡º token æ•°é‡

        Returns:
            æˆæœ¬ï¼ˆç¾Žå…ƒï¼‰
        """
        # ä»·æ ¼è¡¨ï¼ˆæ¯ 1000 tokens çš„ä»·æ ¼ï¼Œå•ä½ï¼šç¾Žå…ƒï¼‰
        PRICING = {
            # OpenAI
            "gpt-4o": {"input": 0.0025, "output": 0.01},
            "gpt-4o-mini": {"input": 0.00015, "output": 0.0006},
            "gpt-4-turbo": {"input": 0.01, "output": 0.03},
            "gpt-4": {"input": 0.03, "output": 0.06},
            "gpt-3.5-turbo": {"input": 0.0005, "output": 0.0015},

            # Anthropic
            "claude-3-5-sonnet-20241022": {"input": 0.003, "output": 0.015},
            "claude-3-opus-20240229": {"input": 0.015, "output": 0.075},
            "claude-3-sonnet-20240229": {"input": 0.003, "output": 0.015},
            "claude-3-haiku-20240307": {"input": 0.00025, "output": 0.00125},

            # DeepSeek
            "deepseek-chat": {"input": 0.0001, "output": 0.0002},
            "deepseek-coder": {"input": 0.0001, "output": 0.0002},
        }

        # èŽ·å–å½“å‰æ¨¡åž‹çš„ä»·æ ¼
        model_name = self.config.model
        pricing = PRICING.get(model_name)

        if not pricing:
            # å¦‚æžœæ‰¾ä¸åˆ°ç²¾ç¡®åŒ¹é…ï¼Œå°è¯•æ¨¡ç³ŠåŒ¹é…
            for key in PRICING.keys():
                if key in model_name or model_name in key:
                    pricing = PRICING[key]
                    break

        if not pricing:
            # ä½¿ç”¨é»˜è®¤ä»·æ ¼ï¼ˆGPT-4oï¼‰
            pricing = PRICING["gpt-4o"]

        # è®¡ç®—æˆæœ¬
        input_cost = (input_tokens / 1000) * pricing["input"]
        output_cost = (output_tokens / 1000) * pricing["output"]

        return input_cost + output_cost

    def get_token_stats(self) -> dict:
        """
        èŽ·å– Token ç»Ÿè®¡ä¿¡æ¯

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        return self.token_stats.copy()

    def reset_token_stats(self):
        """é‡ç½® Token ç»Ÿè®¡"""
        self.token_stats = {
            "total_calls": 0,
            "total_input_tokens": 0,
            "total_output_tokens": 0,
            "total_cost_usd": 0.0
        }

    @classmethod
    def from_env(cls) -> "BuilderClient":
        """Create Builder client from environment variables.
        
        Returns:
            Initialized BuilderClient
        """
        config = BuilderAPIConfig(
            provider=os.getenv("BUILDER_PROVIDER", "openai"),
            model=os.getenv("BUILDER_MODEL", "gpt-4o"),
            api_key=os.getenv("BUILDER_API_KEY", ""),
            base_url=os.getenv("BUILDER_BASE_URL"),
            timeout=int(os.getenv("BUILDER_TIMEOUT", "60")),
            max_retries=int(os.getenv("BUILDER_MAX_RETRIES", "3")),
            temperature=float(os.getenv("BUILDER_TEMPERATURE", "0.7")),
        )
        return cls(config)

