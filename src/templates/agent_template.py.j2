"""
Auto-generated Agent by IteraAgent v8.0
Generated at: {{ timestamp }}
Agent Name: {{ agent_name }}
Description: {{ description }}
Pattern: {{ pattern.pattern_type.value }}
"""

import os
import json
from pathlib import Path
from datetime import datetime
from typing import TypedDict, Annotated, List, Dict, Any, Optional
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver
from langchain_core.messages import HumanMessage, AIMessage, BaseMessage
from langgraph.graph.message import add_messages
{% if has_tools %}
from langchain_core.tools import Tool
{% endif %}
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv
import yaml

{# ğŸ†• æ–¹æ¡ˆ A+: å·¥å…·å¯¼å…¥ (ç½®äºé¡¶éƒ¨,ç¬¦åˆ PEP 8) #}
{% if tool_imports %}
# Tool Imports
{% for import_stmt in tool_imports %}
{{ import_stmt }}
{% endfor %}
{% endif %}

# Load environment variables
load_dotenv()

# Load prompts from YAML
with open("prompts.yaml", "r", encoding="utf-8") as f:
    PROMPTS = yaml.safe_load(f)

# ğŸ†• Helper: Check and prompt for API Keys
def check_api_key(tool_name: str, env_var: str):
    """Check if API Key exists, if not prompt user and save to .env"""
    if not os.getenv(env_var):
        print(f"\nâš ï¸  å·¥å…· {tool_name} éœ€è¦ API Key ({env_var})")
        print(f"   è¯·å‚è€ƒæ–‡æ¡£è·å– Keyã€‚")
        try:
            key = input(f"ğŸ”‘ è¯·è¾“å…¥ {env_var}: ").strip()
            if key:
                # Update current env
                os.environ[env_var] = key
                
                # Update .env file
                env_path = Path(".env")
                if env_path.exists():
                    try:
                        # Try to use dotenv.set_key if available
                        from dotenv import set_key
                        set_key(env_path, env_var, key)
                    except ImportError:
                         # Fallback to appending
                        with open(env_path, "a", encoding="utf-8") as f:
                            f.write(f"\n{env_var}={key}\n")
                else:
                    with open(env_path, "w", encoding="utf-8") as f:
                        f.write(f"{env_var}={key}\n")
                        
                print(f"âœ… å·²ä¿å­˜ {env_var} åˆ° .env")
            else:
                print(f"âš ï¸  æœªæä¾› Key, å·¥å…· {tool_name} å¯èƒ½æ— æ³•æ­£å¸¸å·¥ä½œ")
        except Exception as e:
            print(f"âš ï¸  æ— æ³•ä¿å­˜ Key: {e}")

# ==================== æ ¸å¿ƒï¼šåŠ¨æ€é…ç½®åŠ è½½å™¨ (v7.0 Dynamic Core) ====================
class ConfigLoader:
    """åŠ¨æ€è¯»å– JSON é…ç½®,æ”¯æŒçƒ­æ›´æ–°"""
    def __init__(self):
        self.base_dir = Path(__file__).parent
        self.last_mtime = 0
        self._rag_config_cache = {}
        
    def load_rag_config(self) -> Dict[str, Any]:
        """æ¯æ¬¡è°ƒç”¨æ£€æŸ¥æ–‡ä»¶ä¿®æ”¹æ—¶é—´,å®ç°çƒ­åŠ è½½"""
        config_path = self.base_dir / "rag_config.json"
        
        # ç¼–è¯‘æ—¶çš„é»˜è®¤å€¼ (Fallback)
        defaults = {{ rag_config.model_dump() if rag_config else {} }}
        
        if not config_path.exists():
            return defaults
            
        try:
            current_mtime = config_path.stat().st_mtime
            if current_mtime > self.last_mtime:
                # File changed, reload
                with open(config_path, 'r', encoding='utf-8') as f:
                    self._rag_config_cache = {**defaults, **json.load(f)}
                self.last_mtime = current_mtime
                print(f"ğŸ”„ [Config] RAG é…ç½®å·²çƒ­æ›´æ–° (mtime: {current_mtime})")
            return self._rag_config_cache
        except Exception as e:
            print(f"âš ï¸ Config load failed, using cache/defaults: {e}")
            return self._rag_config_cache or defaults

# å…¨å±€å•ä¾‹
CONFIG_LOADER = ConfigLoader()



# ==================== Trace Manager (Phase 4 - External Storage) ====================
class TraceManager:
    """æ‰§è¡Œè½¨è¿¹ç®¡ç†å™¨ - è´Ÿè´£å¤–éƒ¨å­˜å‚¨,é¿å… Context Window çˆ†ç‚¸
    
    ä¼˜åŒ–ç‚¹:
    - AgentState ä¸­åªå­˜ trace_file è·¯å¾„,ä¸å­˜å®Œæ•´å†…å®¹
    - è¯¦ç»† trace å­˜åˆ° .trace/ ç›®å½•
    - å¤§æ–‡æ¡£å­˜åˆ°å•ç‹¬æ–‡ä»¶ (.trace/docs/)
    """
    
    def __init__(self, agent_dir: Path = None):
        if agent_dir is None:
            agent_dir = Path(__file__).parent
        self.trace_dir = agent_dir / ".trace"
        self.trace_dir.mkdir(exist_ok=True)
        self.current_trace_file = None
        self.trace_entries = []
    
    def start_new_trace(self) -> str:
        """å¼€å§‹æ–°çš„ trace è®°å½•
        
        Returns:
            trace æ–‡ä»¶çš„ç›¸å¯¹è·¯å¾„ (ä¾‹å¦‚: ".trace/run_20260115_123456.json")
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"run_{timestamp}.json"
        self.current_trace_file = self.trace_dir / filename
        self.trace_entries = []
        return str(self.current_trace_file.relative_to(Path.cwd()))
    
    def add_entry(self, entry: Dict[str, Any]):
        """æ·»åŠ  trace æ¡ç›® (åªåœ¨å†…å­˜ä¸­,ä¸ç«‹å³å†™å…¥æ–‡ä»¶)"""
        self.trace_entries.append(entry)
    
    def save(self):
        """ä¿å­˜ trace åˆ°æ–‡ä»¶"""
        if self.current_trace_file:
            with open(self.current_trace_file, 'w', encoding='utf-8') as f:
                json.dump(self.trace_entries, f, indent=2, ensure_ascii=False)
    
    def load(self, trace_file: str) -> List[Dict]:
        """åŠ è½½ trace (ç”¨äºæµ‹è¯•)
        
        Args:
            trace_file: trace æ–‡ä»¶è·¯å¾„ (ç›¸å¯¹æˆ–ç»å¯¹)
        
        Returns:
            å®Œæ•´çš„ trace æ¡ç›®åˆ—è¡¨
        """
        trace_path = Path(trace_file)
        if not trace_path.is_absolute():
            trace_path = Path.cwd() / trace_path
        
        with open(trace_path, 'r', encoding='utf-8') as f:
            return json.load(f)


def _save_docs_to_file(docs: List, step: int) -> str:
    """ä¿å­˜æ–‡æ¡£åˆ°å•ç‹¬æ–‡ä»¶ (é¿å… trace æ–‡ä»¶è¿‡å¤§)
    
    Args:
        docs: æ–‡æ¡£åˆ—è¡¨
        step: å½“å‰æ­¥éª¤ç¼–å·
    
    Returns:
        æ–‡æ¡£æ–‡ä»¶çš„ç›¸å¯¹è·¯å¾„
    """
    docs_dir = Path(__file__).parent / ".trace" / "docs"
    docs_dir.mkdir(parents=True, exist_ok=True)
    
    filename = f"step_{step}_docs.json"
    filepath = docs_dir / filename
    
    # æå–æ–‡æ¡£å†…å®¹
    doc_contents = []
    for doc in docs:
        if hasattr(doc, 'page_content'):
            doc_contents.append(doc.page_content)
        else:
            doc_contents.append(str(doc))
    
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(doc_contents, f, ensure_ascii=False, indent=2)
    
    return str(filepath.relative_to(Path.cwd()))


# å…¨å±€ trace manager
_trace_manager = TraceManager()


# ==================== State Definition ====================
{% if state_schema %}
class AgentState(TypedDict):
    """Agent state structure - Auto-generated from state schema.
    
    Pattern: {{ pattern.pattern_type.value }}
    {% if pattern.description %}
    {{ pattern.description }}
    {% endif %}
    """
{% for field in state_schema.fields %}
    {% if field.reducer %}
    {{ field.name }}: Annotated[{{ field.type.value }}, {{ field.reducer }}]{% if field.description %}  # {{ field.description }}{% endif %}

    {% else %}
    {{ field.name }}: {{ field.type.value }}{% if field.description %}  # {{ field.description }}{% endif %}

    {% endif %}
{% endfor %}
    # ğŸ†• Phase 4: å¤–éƒ¨ Trace å­˜å‚¨ (åªå­˜è·¯å¾„,ä¸å­˜å®Œæ•´å†…å®¹)
    trace_file: Optional[str]  # ä¾‹å¦‚: ".trace/run_20260115_123456.json"

{% else %}
# Fallback state definition
class AgentState(TypedDict):
    """Agent state structure."""
    messages: Annotated[List[BaseMessage], add_messages]
    {% if has_rag %}
    context: str
    retrieved_docs: List[str]
    {% endif %}
    is_finished: bool
    # ğŸ†• Phase 4: å¤–éƒ¨ Trace å­˜å‚¨
    trace_file: Optional[str]
{% endif %}


# ==================== LLM Initialization ====================
llm = ChatOpenAI(
    model=os.getenv("RUNTIME_MODEL", "gpt-3.5-turbo"),
    temperature=float(os.getenv("TEMPERATURE", "0.7")),
    api_key=os.getenv("RUNTIME_API_KEY"),
    base_url=os.getenv("RUNTIME_BASE_URL"),
)

{% if has_rag %}
# ==================== RAG Components ====================

{% include 'rag_embedding.py.j2' %}

{% include 'rag_vectorstore.py.j2' %}

{% include 'rag_document_loader.py.j2' %}

{% include 'rag_retriever.py.j2' %}

{% include 'rag_chain.py.j2' %}

{% endif %}

{% if has_tools %}
# ==================== Tools Initialization ====================
tools = []

{% for tool in tool_inits %}
# Init: {{ tool.name }}
try:
    {% if tool.env_var %}
    if not os.getenv("{{ tool.env_var }}"):
        print(f"âš ï¸  Warning: {{ tool.env_var }} not found. {{ tool.name }} may fail.")
    {% endif %}
    
    tool_instance = {{ tool.class_name }}({{ tool.params }})
    tools.append(tool_instance)
    print(f"âœ… Loaded Tool: {{ tool.name }}")
except Exception as e:
    print(f"âŒ Failed to load {{ tool.name }}: {e}")
    # ç»§ç»­è¿è¡Œ,ä¸é˜»å¡å…¶ä»–å·¥å…·

{% endfor %}
print(f"ğŸ“¦ Total tools loaded: {len(tools)}/{{ tool_inits|length }}")
{% endif %}



# ==================== Node Functions ====================
{% for node in nodes %}
def {{ node.id }}_node(state: AgentState) -> Dict[str, Any]:
    """{{ node.id }} node implementation.
    {% if node.role_description %}
    Role: {{ node.role_description }}
    {% endif %}
    Type: {{ node.type }}
    """
    # ğŸ†• Phase 4: åˆ›å»º trace entry (åªå­˜å…ƒæ•°æ®)
    trace_entry = {
        "step": len(_trace_manager.trace_entries) + 1,
        "node_id": "{{ node.id }}",
        "node_type": "{{ node.type }}",
        "timestamp": datetime.now().isoformat()
    }
    
    {% if node.type == "llm" %}
    {% if node.config and node.config.get('is_router') %}
    # === Intent Router Node (v7.2 Semantic Routing) ===
    messages = state["messages"]
    system_prompt = """{{ node.role_description }}"""
    
    # åªä¼ å…¥ç”¨æˆ·çš„æœ€åä¸€æ¡æ¶ˆæ¯
    user_query = messages[-1].content if messages else ""
    
    response = llm.invoke([
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_query}
    ])
    
    decision = response.content.strip().upper()
    print(f"ğŸš¦ [Router] User: '{user_query[:50]}...' -> Decision: {decision}")
    
    # ğŸ”¥ å…³é”®: ä¿ç•™è¾“å‡ºç”¨äºæ¡ä»¶åˆ¤æ–­,åŒæ—¶å­˜å…¥ä¸“ç”¨å­—æ®µ
    trace_entry.update({
        "action": "intent_routing",
        "user_query": user_query[:100],
        "decision": decision
    })
    _trace_manager.add_entry(trace_entry)
    
    return {
        "messages": [response],  # ğŸ†• ä¿ç•™è¾“å‡ºç”¨äºæ¡ä»¶åˆ¤æ–­
        "router_decision": decision,  # å­˜å…¥ä¸“ç”¨å­—æ®µ
        "trace_file": state.get("trace_file")
    }
    
    {% else %}
    # === Regular LLM Node ===
    messages = state["messages"]
    {% if node.role_description %}
    system_prompt = """{{ node.role_description }}"""
    {% else %}
    system_prompt = PROMPTS.get("system_prompt", "You are a helpful AI assistant.")
    {% endif %}
    
    {% if has_tools %}
    # ç»‘å®šå·¥å…· (ä½¿ LLM èƒ½å¤Ÿç”Ÿæˆ tool_calls)
    llm_with_tools = llm.bind_tools(tools)
    response = llm_with_tools.invoke([
        {"role": "system", "content": system_prompt},
        *messages
    ])
    {% else %}
    response = llm.invoke([
        {"role": "system", "content": system_prompt},
        *messages
    ])
    {% endif %}
    
    # ğŸ†• è®°å½• LLM è°ƒç”¨ (åªå­˜é•¿åº¦å’Œé¢„è§ˆ,ä¸å­˜å®Œæ•´å†…å®¹)
    trace_entry.update({
        "action": "llm_call",
        "input_length": len(messages[-1].content) if messages else 0,
        "output_length": len(response.content),
        "output_preview": response.content[:100]  # åªå­˜å‰100å­—ç¬¦
    })
    _trace_manager.add_entry(trace_entry)
    
    return {
        "messages": [response],
        "trace_file": state.get("trace_file")
    }
    {% endif %}
    
    {% elif node.type == "rag" %}
    # ğŸ”§ v7.2 Fix: è·å–ç”¨æˆ·çš„åŸå§‹æŸ¥è¯¢,è€Œä¸æ˜¯ Router çš„è¾“å‡º
    # é—®é¢˜: å¦‚æœæœ‰ Router èŠ‚ç‚¹,messages[-1] æ˜¯ "SEARCH" è€Œä¸æ˜¯ç”¨æˆ·é—®é¢˜
    # è§£å†³: åªè·å– human ç±»å‹çš„æ¶ˆæ¯
    messages = state["messages"]
    user_query = ""
    
    # ä»åå¾€å‰æ‰¾æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
    for msg in reversed(messages):
        if hasattr(msg, 'type') and msg.type == 'human':
            user_query = msg.content
            break
        elif isinstance(msg, dict) and msg.get("role") == "user":
            user_query = msg.get("content", "")
            break
    
    # å¦‚æœæ²¡æ‰¾åˆ° human æ¶ˆæ¯,å›é€€åˆ°ç¬¬ä¸€æ¡æ¶ˆæ¯
    if not user_query and messages:
        user_query = messages[0].content if hasattr(messages[0], 'content') else str(messages[0])
    
    print(f"ğŸ” [RAG] Query: '{user_query[:50]}...'")
    
    docs = retriever.invoke(user_query)
    context = "\n\n".join([doc.page_content for doc in docs])

    
    # ğŸ†• ä¿å­˜æ–‡æ¡£åˆ°å¤–éƒ¨æ–‡ä»¶ (é¿å… State è¿‡å¤§)
    docs_file = _save_docs_to_file(docs, trace_entry["step"])
    
    rag_prompt = PROMPTS.get("rag_prompt", "Context: {context}\n\nQuestion: {question}").format(
        context=context, 
        question=user_query
    )
    response = llm.invoke(rag_prompt)
    
    # ğŸ†• è®°å½• RAG æ£€ç´¢ (åªå­˜å…ƒæ•°æ®,å®Œæ•´æ–‡æ¡£åœ¨å¤–éƒ¨æ–‡ä»¶)
    trace_entry.update({
        "action": "rag_retrieval",
        "query": user_query,
        "num_docs": len(docs),
        "doc_ids": [f"doc_{i}" for i in range(len(docs))],
        "docs_file": docs_file  # æŒ‡å‘å¤–éƒ¨æ–‡æ¡£æ–‡ä»¶
    })
    _trace_manager.add_entry(trace_entry)

    
    return {
        "messages": [AIMessage(content=response.content)],
        "context": context,
        "retrieved_docs": [doc.page_content for doc in docs],
        "trace_file": state.get("trace_file")
    }
    
    {% elif node.type == "tool" %}
    # Tool execution logic
    # Tool execution logic
    tool_name = "{{ node.config.tool_name if node.config and node.config.tool_name else node.id }}"
    tool_input = state["messages"][-1].content
    
    # helper to find tool
    selected_tool = next((t for t in tools if t.name == tool_name), None)
    
    try:
        if selected_tool:
            # Execute tool
            print(f"ğŸ”§ [Tool] Calling {tool_name} with input: {tool_input[:50]}...")
            tool_output = str(selected_tool.invoke(tool_input))
            print(f"   âœ… Output len: {len(tool_output)}")
        else:
             # Fallback/Mock behavior if tool not found (shouldn't happen if config is correct)
            print(f"âš ï¸ [Tool] {tool_name} not found in initialized tools.")
            tool_output = f"Tool {tool_name} not found. Input was: {tool_input}"
    except Exception as e:
        tool_output = f"Tool execution failed: {e}"
        print(f"âŒ [Tool] Error: {e}")
    
    # ğŸ†• è®°å½•å·¥å…·è°ƒç”¨ (æˆªæ–­é•¿è¾“å‡º)
    trace_entry.update({
        "action": "tool_call",
        "tool_name": tool_name,
        "tool_input": tool_input[:100],  # åªå­˜å‰100å­—ç¬¦
        "tool_output": tool_output[:200]  # åªå­˜å‰200å­—ç¬¦
    })
    _trace_manager.add_entry(trace_entry)
    
    # è·å– tool_call_id (ä»ä¸Šä¸€æ¡æ¶ˆæ¯çš„ tool_calls ä¸­)
    messages = state.get("messages", [])
    tool_call_id = None
    if messages and hasattr(messages[-1], "tool_calls") and messages[-1].tool_calls:
        # æ‰¾åˆ°åŒ¹é…çš„ tool_call
        for tc in messages[-1].tool_calls:
            if tc.get("name") == tool_name or tc["name"] == tool_name:
                tool_call_id = tc.get("id") or tc["id"]
                break
    
    # è¿”å› ToolMessage (OpenAI è¦æ±‚)
    from langchain_core.messages import ToolMessage
    return {
        "messages": [ToolMessage(content=tool_output, tool_call_id=tool_call_id or "unknown")],
        "trace_file": state.get("trace_file")
    }
    
    {% elif node.type == "conditional" %}
    # Conditional logic node
    trace_entry.update({
        "action": "conditional_check",
        "state_snapshot": {k: str(v)[:50] for k, v in state.items() if k != "messages"}
    })
    _trace_manager.add_entry(trace_entry)
    
    return state
    
    {% else %}
    # Custom node
    trace_entry.update({
        "action": "custom_node",
        "note": "Custom node implementation"
    })
    _trace_manager.add_entry(trace_entry)
    
    return state
    {% endif %}

{% endfor %}


# ==================== Condition Functions ====================
{% for cond_edge in conditional_edges %}
def {{ cond_edge.condition }}(state: AgentState) -> str:
    """Condition: {{ cond_edge.condition }}
    {% if cond_edge.condition_logic %}
    
    Logic:
{{ cond_edge.condition_logic | indent(4, first=True) }}
    {% endif %}
    """
    {% if cond_edge.condition_logic %}
    # Auto-generated condition logic
{{ cond_edge.condition_logic | indent(4, first=True) }}
    {% else %}
    # Fallback condition logic
    if state.get("is_finished", False):
        return "end"
    
    # Check iteration count if exists
    if "iteration_count" in state and "max_iterations" in state:
        if state["iteration_count"] >= state["max_iterations"]:
            return "end"
    
    # Default: continue
    return "continue"
    {% endif %}

{% endfor %}


# ==================== Graph Construction ====================
def create_graph():
    """Create and compile the agent graph.
    
    Pattern: {{ pattern.pattern_type.value }}
    Entry Point: {{ entry_point }}
    """
    workflow = StateGraph(AgentState)
    
    # Add nodes
{% for node in nodes %}
    workflow.add_node("{{ node.id }}", {{ node.id }}_node)
{% endfor %}
    
    # Set entry point
    workflow.set_entry_point("{{ entry_point }}")
    
    # Add regular edges
{% for edge in edges %}
    {% if edge.target == "END" %}
    workflow.add_edge("{{ edge.source }}", END)
    {% else %}
    workflow.add_edge("{{ edge.source }}", "{{ edge.target }}")
    {% endif %}
{% endfor %}
    
    # Add conditional edges
{% for cond_edge in conditional_edges %}
    workflow.add_conditional_edges(
        "{{ cond_edge.source }}",
        {{ cond_edge.condition }},
        {
{% for key, value in cond_edge.branches.items() %}
            "{{ key }}": {% if value == "END" %}END{% else %}"{{ value }}"{% endif %},
{% endfor %}
        }
    )
{% endfor %}
    
    # Compile with checkpointer
    memory = MemorySaver()
    return workflow.compile(checkpointer=memory)


# ==================== Helper Function for Testing ====================
def run_agent(user_input: str, return_trace: bool = False):
    """è¿è¡Œ Agent (ç”¨äºæµ‹è¯•)
    
    Args:
        user_input: ç”¨æˆ·è¾“å…¥
        return_trace: æ˜¯å¦è¿”å›æ‰§è¡Œè½¨è¿¹ï¼ˆç”¨äº DeepEval æµ‹è¯•ï¼‰
    
    Returns:
        å¦‚æœ return_trace=False: è¿”å› str (Agent è¾“å‡º)
        å¦‚æœ return_trace=True: è¿”å› (str, List[Dict]) (è¾“å‡º, è½¨è¿¹)
    """
    graph = create_graph()
    
    # ğŸ†• Phase 4: å¼€å§‹æ–°çš„ trace
    trace_file = _trace_manager.start_new_trace()
    
    # å‡†å¤‡åˆå§‹çŠ¶æ€
    initial_state = {
        "messages": [HumanMessage(content=user_input)],
        "trace_file": trace_file,
{% for field in state_schema.fields if field.name != "messages" %}
        "{{ field.name }}": {% if field.default is not none %}{% if field.type.value == 'str' %}"{{ field.default }}"{% else %}{{ field.default }}{% endif %}{% else %}{{ '[]' if 'List' in field.type.value else '{}' if 'Dict' in field.type.value else '0' if field.type.value == 'int' else 'False' if field.type.value == 'bool' else '""' }}{% endif %},
{% endfor %}
    }
    
    # æ‰§è¡Œ graph
    config = {"configurable": {"thread_id": "test"}}
    result = graph.invoke(initial_state, config)
    
    # æå–è¾“å‡º
    output = result["messages"][-1].content if result.get("messages") else ""
    
    # ğŸ†• Phase 4: ä¿å­˜ trace åˆ°æ–‡ä»¶
    _trace_manager.save()
    
    if return_trace:
        # ä»æ–‡ä»¶åŠ è½½å®Œæ•´ trace (ç”¨äºæµ‹è¯•)
        trace = _trace_manager.load(trace_file)
        return output, trace
    
    return output


# ==================== Main Execution ====================
if __name__ == "__main__":
    print("=" * 60)
    print(f"ğŸ¤– {{ agent_name }}")
    print("=" * 60)
    print(f"Description: {{ description }}")
    print(f"Pattern: {{ pattern.pattern_type.value }}")
    {% if pattern.max_iterations > 1 %}
    print(f"Max Iterations: {{ pattern.max_iterations }}")
    {% endif %}
    {% if has_rag %}
    print("\nğŸ“š RAG Mode: Enabled")
    print(f"   Vector Store: {{ rag_config.vector_store }}")
    print(f"   Embedding: {{ rag_config.embedding_provider }}/{{ rag_config.embedding_model_name }}")
    print(f"   Retriever: {{ rag_config.retriever_type }}")
    {% endif %}
    {% if has_tools %}
    print("\nğŸ”§ Tools: {{ enabled_tools | join(', ') }}")
    {% endif %}
    print("=" * 60)
    print("\nType 'quit' or 'q' to exit\n")
    
    graph = create_graph()
    
    # Example usage
    config = {"configurable": {"thread_id": "1"}}
    
    while True:
        user_input = input("You: ").strip()
        if user_input.lower() in ["quit", "exit", "q"]:
            print("\nğŸ‘‹ Goodbye!")
            break
        
        if not user_input:
            continue
        
        try:
            # ğŸ†• Phase 4: å¼€å§‹æ–°çš„ trace
            trace_file = _trace_manager.start_new_trace()
            
            {% if has_rag %}
            # Use RAG for question answering
            result = ask_question(user_input)
            print(f"\nAgent: {result['answer']}\n")
            
            # Display sources
            if result['sources']:
                print("ğŸ“š Sources:")
                for i, source in enumerate(result['sources'][:3], 1):
                    content_preview = source['content'][:100].replace('\n', ' ')
                    print(f"   {i}. {content_preview}...")
                print()
            {% else %}
            # Use graph for general conversation
            initial_state = {
                "messages": [HumanMessage(content=user_input)],
                "trace_file": trace_file,  # ğŸ†• æ·»åŠ  trace_file
{% for field in state_schema.fields if field.name != "messages" %}
                "{{ field.name }}": {% if field.default is not none %}{% if field.type.value == 'str' %}"{{ field.default }}"{% else %}{{ field.default }}{% endif %}{% else %}{{ '[]' if 'List' in field.type.value else '{}' if 'Dict' in field.type.value else '0' if field.type.value == 'int' else 'False' if field.type.value == 'bool' else '""' }}{% endif %},
{% endfor %}
            }
            
            result = graph.invoke(initial_state, config)
            
            # Display response
            if result.get("messages"):
                print(f"\nAgent: {result['messages'][-1].content}\n")
            
            # Display pattern-specific info
            {% if pattern.pattern_type.value == "reflection" %}
            if "iteration_count" in result:
                print(f"   (Iteration: {result['iteration_count']}/{result.get('max_iterations', 3)})")
            {% elif pattern.pattern_type.value == "plan_execute" %}
            if "current_step" in result and "plan" in result:
                print(f"   (Step: {result['current_step']}/{len(result['plan'])})")
            {% endif %}
            {% endif %}
            
            # ğŸ†• Phase 4: ä¿å­˜ trace åˆ°æ–‡ä»¶
            _trace_manager.save()
            print(f"   ğŸ’¾ Trace saved to: {trace_file}")
            
        except Exception as e:
            print(f"\nâŒ Error: {e}\n")
            import traceback
            traceback.print_exc()
