"""
Phase 6 Task 6.5: RAG Optimizer

Optimizes RAG configuration based on test analysis results.
"""

import json
from typing import Optional, Dict, Any
from pathlib import Path

from ..llm.builder_client import BuilderClient
from ..schemas.rag_config import RAGConfig
from ..schemas.analysis_result import AnalysisResult
from ..schemas.test_report import IterationReport


class RAGOptimizer:
    """RAG é…ç½®ä¼˜åŒ–å™¨

    ç»“åˆå¯å‘å¼è§„åˆ™å’Œ LLM æ™ºèƒ½å»ºè®®ä¼˜åŒ– RAG é…ç½®
    """

    def __init__(self, llm_client: BuilderClient):
        """åˆå§‹åŒ–ä¼˜åŒ–å™¨

        Args:
            llm_client: Builder LLM å®¢æˆ·ç«¯
        """
        self.llm = llm_client

    async def optimize_config(
        self, current_config: RAGConfig, analysis: AnalysisResult, test_report: IterationReport
    ) -> RAGConfig:
        """ä¼˜åŒ– RAG é…ç½®

        ç­–ç•¥:
        1. å…ˆåº”ç”¨å¯å‘å¼è§„åˆ™ (å¿«é€Ÿ)
        2. å†ä½¿ç”¨ LLM å¾®è°ƒ (æ™ºèƒ½)

        Args:
            current_config: å½“å‰ RAG é…ç½®
            analysis: LLM åˆ†æç»“æœ
            test_report: æµ‹è¯•æŠ¥å‘Š

        Returns:
            ä¼˜åŒ–åçš„ RAG é…ç½®
        """
        new_config = current_config.model_copy()

        # 1. å¯å‘å¼è§„åˆ™
        if "recall" in analysis.primary_issue.lower():
            # Recall ä½ â†’ å¢åŠ æ£€ç´¢æ–‡æ¡£æ•° æˆ– å¯ç”¨æ··åˆæ£€ç´¢
            current_k = current_config.k_retrieval

            if current_k >= 10 and not current_config.enable_hybrid_search:
                # k å·²ç»å¾ˆå¤§äº†ï¼Œä½† recall è¿˜æ˜¯ä½ -> æ¶æ„å‡çº§: æ··åˆæ£€ç´¢
                new_config.enable_hybrid_search = True
                new_config.k_retrieval = 15  # æ··åˆæ£€ç´¢é€šå¸¸å¯ä»¥å¬å›æ›´å¤š
                print(f"âš¡ [Optimizer] æ¶æ„å‡çº§: æ¿€æ´»æ··åˆæ£€ç´¢ (Hybrid Search)")
            else:
                # ç®€å•å¢åŠ  k
                new_config.k_retrieval = min(current_k * 2, 30)
                print(f"ğŸ“Š å¯å‘å¼è°ƒæ•´: k_retrieval {current_k} â†’ {new_config.k_retrieval}")

        if (
            "precision" in analysis.primary_issue.lower()
            or "faithfulness" in analysis.primary_issue.lower()
        ):
            # Precision/Faithfulness ä½ â†’ å¯ç”¨é‡æ’åº (Rerank)
            if not current_config.reranker_enabled:
                # æ¶æ„å‡çº§: é‡æ’åº
                new_config.reranker_enabled = True
                new_config.reranker_provider = "flashrank"  # é»˜è®¤ä½¿ç”¨è½»é‡çº§
                new_config.k_retrieval = max(
                    current_config.k_retrieval, 10
                )  # Rerank éœ€è¦è¾ƒå¤§çš„å€™é€‰é›†
                print(f"âš¡ [Optimizer] æ¶æ„å‡çº§: æ¿€æ´»é‡æ’åº (Flashrank)")
            else:
                # å·²ç»æœ‰ Rerank äº†ï¼Œå¯èƒ½éœ€è¦æ›´å°çš„ chunk æˆ–æ›´ç²¾å‡†çš„ k
                new_config.chunk_size = max(current_config.chunk_size - 200, 400)
                print(f"ğŸ“Š å¯å‘å¼è°ƒæ•´: chunk_size â†’ {new_config.chunk_size}")

        if "chunk" in analysis.primary_issue.lower():
            # Chunk å¤§å°é—®é¢˜
            pass  # è®© LLM å¤„ç†ï¼Œæˆ–è€…ç®€å•çš„å¯å‘å¼

        # 2. LLM å¾®è°ƒ (å¯é€‰)
        if self.llm:
            try:
                llm_config = await self._llm_optimize(
                    current_config, new_config, analysis, test_report
                )
                # åˆå¹¶ LLM å»ºè®®
                if llm_config:
                    new_config = llm_config
            except Exception as e:
                print(f"âš ï¸ LLM ä¼˜åŒ–å¤±è´¥,ä½¿ç”¨å¯å‘å¼ç»“æœ: {str(e)}")

        return new_config

    async def _llm_optimize(
        self,
        current_config: RAGConfig,
        heuristic_config: RAGConfig,
        analysis: AnalysisResult,
        test_report: IterationReport,
    ) -> Optional[RAGConfig]:
        """ä½¿ç”¨ LLM ä¼˜åŒ–é…ç½®

        Args:
            current_config: å½“å‰é…ç½®
            heuristic_config: å¯å‘å¼è°ƒæ•´åçš„é…ç½®
            analysis: åˆ†æç»“æœ
            test_report: æµ‹è¯•æŠ¥å‘Š

        Returns:
            ä¼˜åŒ–åçš„é…ç½®æˆ– None
        """
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        avg_recall = self._calc_avg_metric(test_report, "contextual_recall")
        avg_faithfulness = self._calc_avg_metric(test_report, "faithfulness")

        prompt = f"""# RAG é…ç½®ä¼˜åŒ–ä»»åŠ¡

## å½“å‰é…ç½®
- chunk_size: {current_config.chunk_size}
- chunk_overlap: {current_config.chunk_overlap}
- k_retrieval: {current_config.k_retrieval}
- retriever_type: {current_config.retriever_type}

## å¯å‘å¼è°ƒæ•´åçš„é…ç½®
- chunk_size: {heuristic_config.chunk_size}
- chunk_overlap: {heuristic_config.chunk_overlap}
- k_retrieval: {heuristic_config.k_retrieval}

## é—®é¢˜åˆ†æ
- ä¸»è¦é—®é¢˜: {analysis.primary_issue}
- æ ¹æœ¬åŸå› : {analysis.root_cause}

## æµ‹è¯•æŒ‡æ ‡
- é€šè¿‡ç‡: {test_report.pass_rate:.1%}
- å¹³å‡ Contextual Recall: {avg_recall:.2f}
- å¹³å‡ Faithfulness: {avg_faithfulness:.2f}

## æœ€ä½³å®è·µçº¦æŸ (Best Practices):

1. **Chunk Size**: 
   - é™¤éæ–‡æ¡£ç»“æ„éå¸¸ç‰¹æ®Šï¼Œå¦åˆ™**ä¸è¦è¶…è¿‡ 600**ã€‚
   - æ¨èèŒƒå›´ï¼š**300 - 500**ã€‚
   - åŸå› ï¼šå°åˆ‡ç‰‡èƒ½æé«˜è¯­ä¹‰å¯†åº¦ï¼Œå‡å°‘å™ªéŸ³ã€‚

2. **Retrieval K**:
   - å¦‚æœ chunk_size è¾ƒå° (<500)ï¼Œå¯ä»¥å¤§èƒ†å¢åŠ  K å€¼ (20-40)ã€‚
   - å¦‚æœ chunk_size è¾ƒå¤§ (>800)ï¼Œå¿…é¡»å‡å° K å€¼ (<10)ã€‚

3. **å¦‚æœ Recall ä½**:
   - ä¼˜å…ˆå‡å° chunk_size (åˆ‡ç¢ä¸€ç‚¹)ï¼ŒåŒæ—¶å¢åŠ  Kã€‚
   - ä¸è¦ç›²ç›®å¢å¤§ chunk_sizeã€‚

## æç«¯æƒ…å†µå¤„ç†æŒ‡å— (Emergency Protocol):

1. **å¦‚æœ Contextual Recall ä¾ç„¶ä¸º 0.0**:
   - æ£€æŸ¥ `k_retrieval`: å¦‚æœå·²ç» > 30ï¼Œ**åœæ­¢å¢åŠ  K å€¼**ï¼ˆå™ªéŸ³å¤ªå¤§ï¼‰ã€‚
   - **å¼ºçƒˆå»ºè®®**: å°† `chunk_size` è°ƒæ•´åˆ° 1000 ä»¥ä¸Šï¼ˆä¿ç•™å®Œæ•´è¯­ä¹‰ï¼‰æˆ–è€… 300 ä»¥ä¸‹ï¼ˆç²¾å‡†åŒ¹é…ï¼‰ã€‚
   - **å¿…é¡»**: åœ¨ `reasoning` ä¸­æŒ‡å‡ºï¼š"å¯èƒ½éœ€è¦ Graph Designer å¢å¼º query_rewriter çš„ Promptï¼Œæˆ–è€…æ£€æŸ¥æºæ–‡æ¡£è§£ææ˜¯å¦ä¸¢å¤±æ•°æ®"ã€‚

2. **å¦‚æœ Faithfulness ä¸º 0.0**:
   - å¿…é¡»å¯ç”¨ `reranker_enabled`ã€‚
   - å‡å° `chunk_size`ã€‚

## ä»»åŠ¡
è¯·ç»™å‡ºæœ€ä¼˜çš„ RAG é…ç½®å‚æ•°ã€‚è€ƒè™‘:
1. å¦‚æœ Recall ä½,å¢åŠ  k_retrieval
2. å¦‚æœ Faithfulness ä½,å¯èƒ½éœ€è¦è°ƒæ•´ chunk_size
3. chunk_overlap é€šå¸¸æ˜¯ chunk_size çš„ 10-20%

è¿”å› JSON:
{{
  "chunk_size": 800,
  "chunk_overlap": 200,
  "k_retrieval": 6,
  "reasoning": "ä¸ºä»€ä¹ˆé€‰æ‹©è¿™äº›å‚æ•°"
}}
"""

        try:
            response = await self.llm.call(prompt)

            # è§£æå“åº”
            import re

            json_match = re.search(r"```json\s*(.*?)\s*```", response, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                json_str = response

            data = json.loads(json_str)

            # æ›´æ–°é…ç½®
            optimized = heuristic_config.model_copy()
            optimized.chunk_size = data.get("chunk_size", optimized.chunk_size)
            optimized.chunk_overlap = data.get("chunk_overlap", optimized.chunk_overlap)
            optimized.k_retrieval = data.get("k_retrieval", optimized.k_retrieval)

            print(f"ğŸ¤– LLM ä¼˜åŒ–å»ºè®®: {data.get('reasoning', 'N/A')}")

            return optimized
        except Exception as e:
            print(f"âš ï¸ LLM ä¼˜åŒ–è§£æå¤±è´¥: {str(e)}")
            return heuristic_config

    def _calc_avg_metric(self, report: IterationReport, metric_name: str) -> float:
        """è®¡ç®—å¹³å‡æŒ‡æ ‡

        Args:
            report: æµ‹è¯•æŠ¥å‘Š
            metric_name: æŒ‡æ ‡åç§°

        Returns:
            å¹³å‡å€¼
        """
        values = []
        for tc in report.test_cases:
            if metric_name in tc.metrics:
                values.append(tc.metrics[metric_name])

        return sum(values) / len(values) if values else 0.0
