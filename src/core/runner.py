"""
Runner - DeepEval æµ‹è¯•æ‰§è¡Œå™¨

è´Ÿè´£:
1. æ£€æŸ¥ DeepEval æ˜¯å¦å·²å®‰è£… (åº”è¯¥åœ¨ Compiler é˜¶æ®µé¢„å®‰è£…)
2. è¿è¡Œ pytest æµ‹è¯•
3. è§£æž JSON æŠ¥å‘Š
4. è¿”å›žæ‰§è¡Œç»“æžœ
5. ðŸ†• Phase 5: æ”¯æŒ HITL æš‚åœ/ç»§ç»­/åœæ­¢æŽ§åˆ¶

ä¼˜åŒ–ç‚¹:
- ä¸å†è¿è¡Œæ—¶å®‰è£… DeepEval (ä¼˜åŒ– 2)
- åªæ£€æŸ¥æ˜¯å¦å·²å®‰è£…,æœªå®‰è£…åˆ™æç¤ºç”¨æˆ·
- ðŸ†• çº¿ç¨‹å®‰å…¨çš„æ‰§è¡ŒæŽ§åˆ¶
"""

import subprocess
import json
import threading
import time
from pathlib import Path
from typing import Optional
from enum import Enum
from queue import Queue
from pydantic import BaseModel, Field

from src.schemas.execution_result import ExecutionResult, ExecutionStatus


class ExecutionControl(Enum):
    """æ‰§è¡ŒæŽ§åˆ¶çŠ¶æ€"""
    RUNNING = "running"
    PAUSED = "paused"
    STOPPED = "stopped"


class DeepEvalTestResult(BaseModel):
    """DeepEval æµ‹è¯•ç»“æžœ"""
    total_tests: int = Field(description="æ€»æµ‹è¯•æ•°")
    passed: int = Field(description="é€šè¿‡æ•°")
    failed: int = Field(description="å¤±è´¥æ•°")
    skipped: int = Field(default=0, description="è·³è¿‡æ•°")
    duration: float = Field(description="æ‰§è¡Œæ—¶é—´(ç§’)")
    test_details: list = Field(default_factory=list, description="æµ‹è¯•è¯¦æƒ…")


class Runner:
    """Agent æ‰§è¡Œå™¨ (DeepEval ç‰ˆæœ¬)

    ä¼˜åŒ–ç‚¹:
    - ä¸å†è¿è¡Œæ—¶å®‰è£… DeepEval
    - ä½¿ç”¨ pytest-json-report èŽ·å–ç»“æž„åŒ–ç»“æžœ
    - ðŸ†• Phase 5: æ”¯æŒ HITL æŽ§åˆ¶ (æš‚åœ/ç»§ç»­/åœæ­¢)
    """

    def __init__(self, agent_dir: Path):
        """åˆå§‹åŒ– Runner

        Args:
            agent_dir: Agent é¡¹ç›®ç›®å½•
        """
        self.agent_dir = Path(agent_dir).absolute()  # ç¡®ä¿ä½¿ç”¨ç»å¯¹è·¯å¾„
        self.venv_python = self._find_python_executable()

        # ðŸ†• Phase 5: HITL æŽ§åˆ¶
        self.control = ExecutionControl.RUNNING
        self.status_queue = Queue()  # çŠ¶æ€é˜Ÿåˆ—ï¼Œä¾› UI è½®è¯¢
        self.log_queue = Queue()     # æ—¥å¿—é˜Ÿåˆ—
        self.current_process: Optional[subprocess.Popen] = None  # å½“å‰è¿è¡Œçš„è¿›ç¨‹
    
    # ðŸ†• Helper to print trace
    def _print_trace(self, agent_dir: Path):
        try:
            # å°è¯•åŠ è½½æœ€æ–°çš„ trace
            trace_dir = agent_dir / ".trace"
            if trace_dir.exists():
                trace_files = sorted(trace_dir.glob("*.json"), key=os.path.getmtime, reverse=True)
                if trace_files:
                    latest_trace = trace_files[0]
                    from src.utils.trace_visualizer import print_trace_summary
                    
                    # Load json
                    with open(latest_trace, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    # Mock SimulationResult object for compatibility if needed, 
                    # or just pass dict if supported.
                    # Assuming print_trace_summary takes a dict or we need to construct object.
                    # For simplicity, let's implement a simple printer here or use the util if adapted.
                    print("\n" + "="*50)
                    print("ðŸ“Š Agent Execution Trace Summary")
                    print("="*50)
                    
                    steps = data.get("steps", [])
                    print(f"Total Steps: {len(steps)}")
                    print(f"Status: {'âœ… Success' if data.get('success') else 'âŒ Failed'}")
                    
                    print("\nExecution Flow:")
                    for step in steps:
                        icon = "âœ…" if step.get('step_type') == 'success' else "âŒ" if step.get('step_type') == 'failed' else "âž¡ï¸"
                        print(f"  {icon} [{step.get('node_id')}] {step.get('description')}")
                        if step.get('tool_calls'):
                             for tc in step['tool_calls']:
                                 print(f"     ðŸ”¨ Tool: {tc.get('tool_name')}")
                    
                    if data.get("issues"):
                        print("\nâš ï¸  Issues Detected:")
                        for issue in data['issues']:
                            print(f"  - [{issue.get('severity')}] {issue.get('description')}")
                    print("="*50 + "\n")
                    
        except Exception as e:
            print(f"âš ï¸ Failed to print trace summary: {e}")
    
    def _find_python_executable(self) -> Path:
        """æŸ¥æ‰¾ Python å¯æ‰§è¡Œæ–‡ä»¶
        
        Returns:
            Python å¯æ‰§è¡Œæ–‡ä»¶è·¯å¾„
        """
        # ðŸ†• Debug: æ˜¾ç¤º agent_dir
        print(f"ðŸ” [Runner] Agent ç›®å½•: {self.agent_dir}")
        print(f"ðŸ” [Runner] Agent ç›®å½•å­˜åœ¨: {self.agent_dir.exists()}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è™šæ‹ŸçŽ¯å¢ƒ
        venv_paths = [
            self.agent_dir / "venv" / "Scripts" / "python.exe",  # Windows
            self.agent_dir / "venv" / "bin" / "python",  # Linux/Mac
        ]
        
        for venv_path in venv_paths:
            # ðŸ†• Debug: æ˜¾ç¤ºæ¯ä¸ªæ£€æŸ¥çš„è·¯å¾„
            print(f"ðŸ” [Runner] æ£€æŸ¥è·¯å¾„: {venv_path}")
            print(f"ðŸ” [Runner] è·¯å¾„å­˜åœ¨: {venv_path.exists()}")
            
            if venv_path.exists():
                print(f"âœ… [Runner] æ‰¾åˆ° venv Python: {venv_path}")
                return venv_path
        
        # ä½¿ç”¨ç³»ç»Ÿ Python
        import sys
        print(f"âš ï¸ [Runner] æœªæ‰¾åˆ° venv,ä½¿ç”¨ç³»ç»Ÿ Python: {sys.executable}")
        return Path(sys.executable)

    # ðŸ†• Phase 5: HITL æŽ§åˆ¶æ–¹æ³•
    def pause(self):
        """æš‚åœæ‰§è¡Œ"""
        self.control = ExecutionControl.PAUSED
        self.status_queue.put({"status": "paused", "message": "æ‰§è¡Œå·²æš‚åœ"})
        self.log_queue.put({"level": "WARNING", "message": "æ‰§è¡Œå·²æš‚åœ"})

    def resume(self):
        """ç»§ç»­æ‰§è¡Œ"""
        self.control = ExecutionControl.RUNNING
        self.status_queue.put({"status": "running", "message": "æ‰§è¡Œå·²ç»§ç»­"})
        self.log_queue.put({"level": "INFO", "message": "æ‰§è¡Œå·²ç»§ç»­"})

    def stop(self):
        """åœæ­¢æ‰§è¡Œ"""
        self.control = ExecutionControl.STOPPED
        self.status_queue.put({"status": "stopped", "message": "æ‰§è¡Œå·²åœæ­¢"})
        self.log_queue.put({"level": "ERROR", "message": "æ‰§è¡Œå·²åœæ­¢"})

        # ç»ˆæ­¢å½“å‰è¿›ç¨‹
        if self.current_process and self.current_process.poll() is None:
            self.current_process.terminate()
            try:
                self.current_process.wait(timeout=5)
            except subprocess.TimeoutExpired:
                self.current_process.kill()

    def get_status(self) -> str:
        """èŽ·å–å½“å‰çŠ¶æ€"""
        return self.control.value

    def _check_control_state(self):
        """æ£€æŸ¥æŽ§åˆ¶çŠ¶æ€ï¼ˆåœ¨å…³é”®ç‚¹è°ƒç”¨ï¼‰"""
        # å¦‚æžœæš‚åœï¼Œç­‰å¾…æ¢å¤
        while self.control == ExecutionControl.PAUSED:
            time.sleep(0.1)

        # å¦‚æžœåœæ­¢ï¼ŒæŠ›å‡ºå¼‚å¸¸
        if self.control == ExecutionControl.STOPPED:
            raise RuntimeError("æ‰§è¡Œå·²è¢«ç”¨æˆ·åœæ­¢")
    
    def setup_environment(self) -> bool:
        """è®¾ç½®è¿è¡ŒçŽ¯å¢ƒ (å®‰è£…ä¾èµ–)"""
        install_script = "install.bat" if subprocess.os.name == "nt" else "./install.sh"
        script_path = self.agent_dir / install_script
        
        if not script_path.exists():
            return False
            
        try:
            cmd = str(script_path.absolute()) if subprocess.os.name == "nt" else str(script_path.absolute())
            # For subprocess run, we might need shell=True on windows for bat files? 
            # Usually .bat needs shell=True or direct full path execution.
            print(f"Executing {cmd}...")
            subprocess.run(
                [cmd], 
                cwd=str(self.agent_dir), 
                check=True, 
                shell=(subprocess.os.name == "nt")
            )
            return True
        except Exception as e:
            print(f"Installation failed: {e}")
            return False
    
    def run_deepeval_tests(
        self,
        test_file: str = "tests/test_deepeval.py",
        timeout: int = 1200
    ) -> ExecutionResult:
        """è¿è¡Œ DeepEval æµ‹è¯•
        
        Args:
            test_file: æµ‹è¯•æ–‡ä»¶è·¯å¾„ (ç›¸å¯¹äºŽ agent_dir)
            timeout: è¶…æ—¶æ—¶é—´(ç§’)
        
        Returns:
            ExecutionResult åŒ…å«æµ‹è¯•ç»“æžœ
        """
        # ðŸ†• ä¼˜åŒ–: æ£€æŸ¥ DeepEval æ˜¯å¦å·²å®‰è£… (ä¸å†è¿è¡Œæ—¶å®‰è£…)
        if not self._check_deepeval_installed():
            return ExecutionResult(
                overall_status=ExecutionStatus.ERROR,
                test_results=[],
                stderr="DeepEval æœªå®‰è£…! è¯·å…ˆè¿è¡Œå®‰è£…è„šæœ¬:\n"
                      "  Linux/Mac: ./install.sh\n"
                      "  Windows: install.bat\n"
                      "æˆ–æ‰‹åŠ¨å®‰è£…: pip install -r requirements.txt"
            )
        
        # æ£€æŸ¥æµ‹è¯•æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        test_path = self.agent_dir / test_file
        if not test_path.exists():
            return ExecutionResult(
                overall_status=ExecutionStatus.ERROR,
                test_results=[],
                stderr=f"æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_file}"
            )
        
        # è¿è¡Œ pytest
        try:
            result = self._run_pytest(test_file, timeout)
            
            # ðŸ†• Print trace summary after execution
            self._print_trace(self.agent_dir)
            
            return result
        except subprocess.TimeoutExpired:
            return ExecutionResult(
                overall_status=ExecutionStatus.TIMEOUT,
                test_results=[],
                stderr=f"æµ‹è¯•æ‰§è¡Œè¶…æ—¶ ({timeout}ç§’)"
            )
        except Exception as e:
            return ExecutionResult(
                overall_status=ExecutionStatus.ERROR,
                test_results=[],
                stderr=f"æµ‹è¯•æ‰§è¡Œå¤±è´¥: {str(e)}"
            )
    
    def _check_deepeval_installed(self) -> bool:
        """ðŸ†• æ£€æŸ¥ DeepEval æ˜¯å¦å·²å®‰è£…
        
        Returns:
            True if installed, False otherwise
        """
        try:
            # ä½¿ç”¨ venv ä¸­çš„ Python æ£€æŸ¥
            print(f"ðŸ” [Runner] æ£€æŸ¥ Python è·¯å¾„: {self.venv_python}")
            print(f"ðŸ” [Runner] Python æ˜¯å¦å­˜åœ¨: {self.venv_python.exists()}")
            
            result = subprocess.run(
                [str(self.venv_python), "-c", "import deepeval; print('OK')"],
                cwd=self.agent_dir,
                capture_output=True,
                text=True,
                timeout=60  # ðŸ”§ å¢žåŠ åˆ° 60 ç§’ (é¦–æ¬¡å¯¼å…¥ deepeval å¯èƒ½éœ€è¦ä¸‹è½½æ¨¡åž‹)
            )
            
            # ðŸ†• Debug logging
            print(f"ðŸ” [Runner] DeepEval æ£€æŸ¥:")
            print(f"   - è¿”å›žç : {result.returncode}")
            print(f"   - Stdout: {result.stdout.strip()}")
            if result.stderr:
                print(f"   - Stderr: {result.stderr.strip()}")
            
            return result.returncode == 0 and "OK" in result.stdout
        except subprocess.TimeoutExpired:
            print(f"ðŸ” [Runner] DeepEval æ£€æŸ¥è¶…æ—¶ (60ç§’)")
            return False
        except Exception as e:
            print(f"ðŸ” [Runner] DeepEval æ£€æŸ¥å¤±è´¥: {e}")
            return False
    
    def _run_pytest(self, test_file: str, timeout: int) -> ExecutionResult:
        """è¿è¡Œ pytest å¹¶è§£æžç»“æžœ
        
        Args:
            test_file: æµ‹è¯•æ–‡ä»¶è·¯å¾„
            timeout: è¶…æ—¶æ—¶é—´
        
        Returns:
            ExecutionResult
        """
        import time
        start_time = time.time()
        
        # æž„é€  pytest å‘½ä»¤
        report_file = self.agent_dir / "deepeval_results.json"
        cmd = [
            str(self.venv_python),
            "-m", "pytest",
            test_file,
            "--json-report",
            f"--json-report-file={report_file.name}",
            "-v", "-s"
        ]
        
        # ðŸ†• Debug logging
        print(f"ðŸ” [Runner] æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
        print(f"ðŸ” [Runner] å·¥ä½œç›®å½•: {self.agent_dir}")
        print(f"ðŸ” [Runner] Python: {self.venv_python}")
        
        # è¿è¡Œå‘½ä»¤
        result = subprocess.run(
            cmd,
            cwd=self.agent_dir,
            capture_output=True,
            text=True,
            timeout=timeout
        )
        
        execution_time = time.time() - start_time
        
        # ðŸ†• Debug logging
        print(f"ðŸ” [Runner] è¿”å›žç : {result.returncode}")
        print(f"ðŸ” [Runner] æ‰§è¡Œæ—¶é—´: {execution_time:.2f}s")
        print(f"ðŸ” [Runner] Stderr: {result.stderr[:300] if result.stderr else 'None'}")
        
        # è§£æž JSON æŠ¥å‘Š
        if report_file.exists():
            print(f"ðŸ” [Runner] âœ… æŠ¥å‘Šæ–‡ä»¶å­˜åœ¨: {report_file}")
            print(f"ðŸ” [Runner] æŠ¥å‘Šæ–‡ä»¶å¤§å°: {report_file.stat().st_size} bytes")
            
            try:
                with open(report_file, 'r', encoding='utf-8') as f:
                    report_data = json.load(f)
                
                print(f"ðŸ” [Runner] JSON è§£æžæˆåŠŸ")
                print(f"ðŸ” [Runner] æŠ¥å‘Šé”®: {list(report_data.keys())}")
                
                # æ˜¾ç¤º summary ä¿¡æ¯
                if 'summary' in report_data:
                    print(f"ðŸ” [Runner] Summary: {report_data['summary']}")
                
                # æ˜¾ç¤ºæµ‹è¯•æ•°é‡
                if 'tests' in report_data:
                    print(f"ðŸ” [Runner] æµ‹è¯•æ•°é‡: {len(report_data['tests'])}")
                    if report_data['tests']:
                        print(f"ðŸ” [Runner] ç¬¬ä¸€ä¸ªæµ‹è¯•: {report_data['tests'][0].get('nodeid', 'unknown')}")
                
                test_result = self._parse_json_report(report_file)
                
                print(f"ðŸ” [Runner] è§£æžç»“æžœç±»åž‹: {type(test_result)}")
                print(f"ðŸ” [Runner] è§£æžæˆåŠŸ - Status: {test_result.overall_status}, Tests: {len(test_result.test_results)}")
                
                return test_result
                
            except Exception as e:
                print(f"ðŸ” [Runner] âŒ JSONè§£æžå¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                # JSON è§£æžå¤±è´¥,å›žé€€åˆ° stdout è§£æž
                return self._parse_pytest_stdout(
                    result.stdout,
                    result.stderr,
                    execution_time
                )
        else:
            # æ²¡æœ‰ JSON æŠ¥å‘Š,è§£æž stdout
            return self._parse_pytest_stdout(
                result.stdout,
                result.stderr,
                execution_time
            )
    
    def _parse_json_report(self, report_file: Path) -> "DeepEvalTestResult":
        """è§£æž pytest-json-report ç”Ÿæˆçš„ JSON æ–‡ä»¶
        
        Args:
            report_file: JSON æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
            
        Returns:
            DeepEvalTestResult å¯¹è±¡
        """
        with open(report_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        print(f"ðŸ” [_parse_json_report] å¼€å§‹è§£æž JSON æŠ¥å‘Š")
        print(f"ðŸ” [_parse_json_report] æŠ¥å‘Šé”®: {list(data.keys())}")
        
        # æå–æ±‡æ€»ä¿¡æ¯
        summary = data.get('summary', {})
        print(f"ðŸ” [_parse_json_report] Summary å†…å®¹: {summary}")
        
        total = summary.get('total', 0)
        passed = summary.get('passed', 0)
        failed = summary.get('failed', 0)
        skipped = summary.get('skipped', 0)
        duration = data.get('duration', 0.0)
        tests = data.get('tests', [])
        
        print(f"ðŸ” [_parse_json_report] ç»Ÿè®¡: Total={total}, Passed={passed}, Failed={failed}, Skipped={skipped}, Duration={duration:.2f}s")
        print(f"ðŸ” [_parse_json_report] å‘çŽ° {len(tests)} ä¸ªæµ‹è¯•è¯¦æƒ…")
        
        # åˆ›å»º TestResult åˆ—è¡¨
        from ..schemas.execution_result import TestResult, ExecutionStatus
        
        test_results = []
        for test in tests:
            # ç¡®å®šçŠ¶æ€
            outcome = test.get('outcome', 'failed')
            if outcome == 'passed':
                status = ExecutionStatus.PASS
            elif outcome == 'failed':
                status = ExecutionStatus.FAIL
            elif outcome == 'skipped':
                status = ExecutionStatus.SKIPPED
            else:
                status = ExecutionStatus.ERROR
            
            # æå–é”™è¯¯ä¿¡æ¯
            error_msg = None
            if 'call' in test and 'crash' in test['call']:
                error_msg = test['call']['crash'].get('message', '')
            
            test_results.append(TestResult(
                test_id=test.get('nodeid', 'unknown'),
                status=status,
                actual_output=None,
                error_message=error_msg,
                duration_ms=int(test.get('call', {}).get('duration', 0) * 1000) if 'call' in test else 0
            ))
        
        # ç¡®å®šæ•´ä½“çŠ¶æ€
        if failed == 0 and total > 0:
            overall_status = ExecutionStatus.SUCCESS
        elif total == 0:
            overall_status = ExecutionStatus.ERROR
        else:
            overall_status = ExecutionStatus.FAILED
        
        print(f"ðŸ” [_parse_json_report] åˆ›å»º ExecutionResult: status={overall_status}, total={total}")
        
        from ..schemas.execution_result import ExecutionResult
        return ExecutionResult(
            overall_status=overall_status,
            test_results=test_results,
            stderr=None
        )
    
    def _parse_pytest_stdout(
        self,
        stdout: str,
        stderr: str,
        execution_time: float
    ) -> ExecutionResult:
        """è§£æž pytest çš„ stdout è¾“å‡º (å›žé€€æ–¹æ¡ˆ)
        
        Args:
            stdout: æ ‡å‡†è¾“å‡º
            stderr: æ ‡å‡†é”™è¯¯
            execution_time: æ‰§è¡Œæ—¶é—´
        
        Returns:
            ExecutionResult
        """
        # ç®€å•çš„å¯å‘å¼è§£æž
        passed = stdout.count(" PASSED")
        failed = stdout.count(" FAILED")
        
        if failed == 0 and passed > 0:
            status = ExecutionStatus.SUCCESS
        elif failed > 0:
            status = ExecutionStatus.FAILED
        else:
            status = ExecutionStatus.ERROR
        
        return ExecutionResult(
            overall_status=status,  # ä½¿ç”¨ overall_status
            test_results=[],  # ç®€åŒ–ç‰ˆ
            stderr=stderr if stderr else None
        )
