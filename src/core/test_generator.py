"""
Test Generator - DeepEval ç‰ˆæœ¬

ç”Ÿæˆä¸“ä¸šçš„ DeepEval æµ‹è¯•ä»£ç ,æ”¯æŒ:
1. RAG Fact-based æµ‹è¯• (ä½¿ç”¨å¤–éƒ¨ Trace)
2. Logic G-Eval æµ‹è¯• (éªŒè¯å·¥å…·è°ƒç”¨)
3. ç®€åŒ–çš„ Ollama é›†æˆ (ä½¿ç”¨å®˜æ–¹æ¥å£)
"""

from typing import List, Optional, Dict, Any
from pydantic import BaseModel, Field, ConfigDict
from pathlib import Path
import json

from src.llm.builder_client import BuilderClient
from src.schemas.project_meta import ProjectMeta, TaskType
from src.schemas.rag_config import RAGConfig


class DeepEvalTestConfig(BaseModel):
    """DeepEval æµ‹è¯•é…ç½®"""
    num_rag_tests: int = Field(default=5, ge=1, le=20, description="RAG æµ‹è¯•æ•°é‡")
    num_logic_tests: int = Field(default=3, ge=1, le=10, description="Logic æµ‹è¯•æ•°é‡")
    use_local_llm: bool = Field(default=True, description="ä½¿ç”¨æœ¬åœ° Ollama")
    judge_model: str = Field(default="llama3", description="è¯„ä¼°ç”¨çš„æ¨¡å‹")
    deepeval_version: str = Field(default="0.21.0", description="DeepEval ç‰ˆæœ¬")
    
model_config = ConfigDict(
        arbitrary_types_allowed=True,
        json_schema_extra={
            "example": {
                "num_rag_tests": 5,
                "num_logic_tests": 3,
                "use_local_llm": True,
                "judge_model": "llama3",
                "deepeval_version": "0.21.0"
            }
        }
    )


class TestGenerator:
    """DeepEval æµ‹è¯•ç”Ÿæˆå™¨ (ä¼˜åŒ–ç‰ˆ)
    
    ä¼˜åŒ–ç‚¹:
    1. ä½¿ç”¨å¤–éƒ¨ Trace æ–‡ä»¶ (ä¸å ç”¨ Context Window)
    2. ç®€åŒ– Ollama é›†æˆ (ä½¿ç”¨ ChatOllama,ä¸è‡ªå®šä¹‰ç±»)
    3. æ”¯æŒå¯å‘å¼å›é€€ (LLM å¤±è´¥æ—¶)
    """
    
    def __init__(self, llm_client: BuilderClient):
        """åˆå§‹åŒ–æµ‹è¯•ç”Ÿæˆå™¨
        
        Args:
            llm_client: Builder LLM å®¢æˆ·ç«¯ (ç”¨äºç”Ÿæˆæµ‹è¯•ç”¨ä¾‹)
        """
        self.llm = llm_client
    
    async def generate_deepeval_tests(
        self,
        project_meta: ProjectMeta,
        rag_config: Optional[RAGConfig] = None,
        config: DeepEvalTestConfig = DeepEvalTestConfig()
    ) -> str:
        """ç”Ÿæˆå®Œæ•´çš„ DeepEval æµ‹è¯•æ–‡ä»¶
        
        Args:
            project_meta: é¡¹ç›®å…ƒä¿¡æ¯
            rag_config: RAG é…ç½® (å¦‚æœæœ‰)
            config: æµ‹è¯•é…ç½®
        
        Returns:
            å®Œæ•´çš„ test_deepeval.py æ–‡ä»¶å†…å®¹
        """
        sections = []
        
        # 1. å¯¼å…¥è¯­å¥
        sections.append(self._generate_imports(config))
        
        # 2. é…ç½® DeepEval (ä¼˜åŒ–ç‰ˆ - ç®€åŒ– Ollama é›†æˆ)
        sections.append(self._generate_deepeval_config_optimized(config))
        
        # 3. RAG æµ‹è¯• (å¦‚æœæœ‰ RAG)
        if rag_config and project_meta.has_rag:
            rag_tests = await self._generate_rag_tests(
                project_meta, rag_config, config.num_rag_tests
            )
            sections.append(rag_tests)
        
        # 4. Logic æµ‹è¯•
        logic_tests = await self._generate_logic_tests(
            project_meta, config.num_logic_tests
        )
        sections.append(logic_tests)
        
        return "\n\n".join(sections)
    
    def _generate_imports(self, config: DeepEvalTestConfig) -> str:
        """ç”Ÿæˆå¯¼å…¥è¯­å¥"""
        return f'''"""
Auto-generated DeepEval tests by Agent Zero
Generated with DeepEval v{config.deepeval_version}
"""
from deepeval import assert_test
from deepeval.test_case import LLMTestCase
from deepeval.metrics import (
    FaithfulnessMetric,
    AnswerRelevancyMetric,
    ContextualRecallMetric,
    ContextualPrecisionMetric,
    GEval
)
from deepeval.test_case import LLMTestCaseParams
import sys
import json
from pathlib import Path
from unittest.mock import MagicMock, patch

# å¯¼å…¥ Agent
sys.path.insert(0, "..")
from agent import run_agent
import agent
'''
    
    def _generate_deepeval_config_optimized(self, config: DeepEvalTestConfig) -> str:
        """ç”Ÿæˆ DeepEval é…ç½® (ä¼˜åŒ–ç‰ˆ - ä½¿ç”¨å®˜æ–¹æ¥å£)
        
        ä¼˜åŒ–ç‚¹:
        - ä¸å†è‡ªå®šä¹‰ OllamaModel ç±» (~150 è¡Œä»£ç )
        - ç›´æ¥ä½¿ç”¨ ChatOllama (~10 è¡Œä»£ç )
        - DeepEval ä¼šè‡ªåŠ¨é€‚é… LangChain æ¨¡å‹
        """
        if config.use_local_llm:
            return f'''
# ==================== DeepEval é…ç½® (åŠ¨æ€é€‚é…ç‰ˆ) ====================
import os
from deepeval.models import DeepEvalBaseLLM
from langchain_community.chat_models import ChatOpenAI, ChatOllama

class AgentZeroJudge(DeepEvalBaseLLM):
    """ç»Ÿä¸€çš„ Agent Zero è¯„åˆ¤æ¨¡å‹é€‚é…å™¨
    
    æ”¯æŒ:
    1. OpenAI å…¼å®¹æ¥å£ (DeepSeek, GPT-4) - ä¼˜å…ˆ
    2. Ollama æœ¬åœ°æ¨¡å‹ - å›é€€
    """
    def __init__(self):
        # 1. å°è¯•è¯»å– JUDGE æˆ– RUNTIME ç¯å¢ƒå˜é‡
        self.api_key = os.getenv("JUDGE_API_KEY") or os.getenv("RUNTIME_API_KEY")
        self.base_url = os.getenv("JUDGE_BASE_URL") or os.getenv("RUNTIME_BASE_URL")
        self.model_name = os.getenv("JUDGE_MODEL") or os.getenv("RUNTIME_MODEL") or "deepseek-chat"
        self.provider = os.getenv("JUDGE_PROVIDER") or "openai"
        
        # 2. åˆ¤æ–­é€šè¿‡å“ªç§æ–¹å¼åˆå§‹åŒ–
        if self.api_key and "sk-" in self.api_key:
            print(f"âš–ï¸  DeepEval Judge: ä½¿ç”¨äº‘ç«¯æ¨¡å‹ ({{self.model_name}})")
            self.llm = ChatOpenAI(
                model=self.model_name,
                openai_api_key=self.api_key,
                openai_api_base=self.base_url,
                temperature=0.0
            )
            self._is_local = False
        else:
            print(f"âš–ï¸  DeepEval Judge: ä½¿ç”¨æœ¬åœ° Ollama ({config.judge_model})")
            self.llm = ChatOllama(
                model="{config.judge_model}",
                base_url="http://localhost:11434",
                temperature=0.0,
                format="json"  # å¼ºåˆ¶ JSON
            )
            self._is_local = True

    def load_model(self):
        return self.llm

    def generate(self, prompt: str) -> str:
        return self.llm.invoke(prompt).content

    async def a_generate(self, prompt: str) -> str:
        res = await self.llm.ainvoke(prompt)
        return res.content
        
    def get_model_name(self):
        return self.model_name

# å…¨å±€è¯„åˆ¤å®ä¾‹
judge_llm = AgentZeroJudge()
'''
        else:
            return '''
# ==================== DeepEval é…ç½® ====================
# ä½¿ç”¨é»˜è®¤ OpenAI æ¨¡å‹
import os
if not os.getenv("OPENAI_API_KEY"):
    raise ValueError("è¯·è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")

judge_llm = None  # DeepEval ä¼šä½¿ç”¨é»˜è®¤ OpenAI æ¨¡å‹
'''
    
    async def _generate_rag_tests(
        self,
        project_meta: ProjectMeta,
        rag_config: RAGConfig,
        num_tests: int
    ) -> str:
        """ç”Ÿæˆ RAG æµ‹è¯• (Fact-based,ä½¿ç”¨å¤–éƒ¨ Trace)
        
        Args:
            project_meta: é¡¹ç›®å…ƒä¿¡æ¯ (åŒ…å« file_paths)
            rag_config: RAG é…ç½®
            num_tests: æµ‹è¯•æ•°é‡
        
        Returns:
            RAG æµ‹è¯•å‡½æ•°ä»£ç 
        """
        # 1. ä»æ–‡æ¡£æå–é—®ç­”å¯¹ (file_paths åœ¨ project_meta ä¸­)
        file_paths = project_meta.file_paths or []
        qa_pairs = await self._extract_qa_from_docs(
            file_paths, num_tests
        )
        
        # 2. ç”Ÿæˆæµ‹è¯•å‡½æ•°
        test_functions = []
        for i, qa in enumerate(qa_pairs, 1):
            test_func = f'''
def test_rag_fact_{i}():
    """æµ‹è¯• RAG Fact {i}: {qa['question'][:50]}..."""
    query = """{qa['question']}"""
    
    # è¿è¡Œ Agent (è·å– trace)
    output, trace = run_agent(query, return_trace=True)
    
    # ğŸ†• ä»å¤–éƒ¨ trace æ–‡ä»¶æå–æ£€ç´¢å†…å®¹
    rag_steps = [s for s in trace if s.get("action") == "rag_retrieval"]
    retrieved_docs = []
    if rag_steps:
        # åŠ è½½å®Œæ•´æ–‡æ¡£å†…å®¹ (ä»å¤–éƒ¨æ–‡ä»¶)
        docs_file = rag_steps[0].get("docs_file")
        if docs_file:
            with open(docs_file, 'r', encoding='utf-8') as f:
                retrieved_docs = json.load(f)
    
    # æ„é€ æµ‹è¯•ç”¨ä¾‹
    test_case = LLMTestCase(
        input=query,
        actual_output=output,
        retrieval_context=retrieved_docs,
        expected_output="""{qa['expected_answer']}"""
    )
    
    # å®šä¹‰æŒ‡æ ‡ (ğŸ†• ç›´æ¥ä½¿ç”¨ ChatOllama)
    faithfulness = FaithfulnessMetric(
        threshold=0.7,
        model=judge_llm,
        include_reason=True
    )
    recall = ContextualRecallMetric(
        threshold=0.8,
        model=judge_llm
    )
    
    # æ–­è¨€
    assert_test(test_case, [faithfulness, recall])
    print(f"âœ… RAG Fact {i} æµ‹è¯•é€šè¿‡")
'''
            test_functions.append(test_func)
        
        header = f'''
# ==================== RAG Fact-based æµ‹è¯• ====================
# ä»æ–‡æ¡£ä¸­æå–çš„äº‹å®æ€§é—®é¢˜,éªŒè¯ RAG å‡†ç¡®æ€§
# ä½¿ç”¨æŒ‡æ ‡: Faithfulness (å¿ å®åº¦), ContextualRecall (å¬å›ç‡)
'''
        return header + "\n".join(test_functions)
    
    async def _extract_qa_from_docs(
        self,
        file_paths: List[str],
        num_tests: int
    ) -> List[Dict[str, str]]:
        """ä»æ–‡æ¡£æå–é—®ç­”å¯¹ (ä½¿ç”¨ LLM)
        
        Args:
            file_paths: æ–‡æ¡£è·¯å¾„åˆ—è¡¨
            num_tests: éœ€è¦æå–çš„é—®ç­”å¯¹æ•°é‡
        
        Returns:
            é—®ç­”å¯¹åˆ—è¡¨ [{"question": "...", "expected_answer": "..."}]
        """
        try:
            print(f"ğŸ” [è°ƒè¯•] å¼€å§‹æå–é—®ç­”å¯¹: {num_tests} ä¸ª, æ–‡æ¡£æ•°: {len(file_paths)}")
            
            # 1. åŠ è½½æ–‡æ¡£å†…å®¹
            print(f"ğŸ“„ [è°ƒè¯•] æ­¥éª¤ 1/5: åŠ è½½æ–‡æ¡£...")
            document_content = await self._load_documents(file_paths)
            print(f"âœ… [è°ƒè¯•] æ–‡æ¡£åŠ è½½æˆåŠŸ, é•¿åº¦: {len(document_content)} å­—ç¬¦")
            
            # 2. åŠ è½½ Prompt æ¨¡æ¿
            print(f"ğŸ“ [è°ƒè¯•] æ­¥éª¤ 2/5: åŠ è½½ Prompt æ¨¡æ¿...")
            prompt_template = self._load_prompt_template("test_generator_deepeval_rag.txt")
            print(f"âœ… [è°ƒè¯•] Prompt æ¨¡æ¿åŠ è½½æˆåŠŸ, é•¿åº¦: {len(prompt_template)} å­—ç¬¦")
            
            # 3. æ„é€  Prompt
            print(f"ğŸ”§ [è°ƒè¯•] æ­¥éª¤ 3/5: æ„é€  Prompt...")
            prompt = prompt_template.format(
                num_tests=num_tests,
                document_content=document_content[:10000]  # é™åˆ¶é•¿åº¦,é¿å…è¶…å‡º Context Window
            )
            print(f"âœ… [è°ƒè¯•] Prompt æ„é€ æˆåŠŸ, é•¿åº¦: {len(prompt)} å­—ç¬¦")
            
            # 4. è°ƒç”¨ LLM
            print(f"ğŸ¤– [è°ƒè¯•] æ­¥éª¤ 4/5: è°ƒç”¨ LLM ç”Ÿæˆé—®ç­”å¯¹...")
            response = await self.llm.call(prompt)  # ä½¿ç”¨ call() è€Œé generate()
            print(f"âœ… [è°ƒè¯•] LLM å“åº”æˆåŠŸ, é•¿åº¦: {len(response)} å­—ç¬¦")
            print(f"ğŸ“‹ [è°ƒè¯•] LLM å“åº”é¢„è§ˆ (å‰ 200 å­—ç¬¦):\n{response[:200]}...")
            
            # 5. è§£æ JSON å“åº”
            print(f"ğŸ” [è°ƒè¯•] æ­¥éª¤ 5/5: è§£æ JSON å“åº”...")
            qa_pairs = self._parse_json_response(response)
            print(f"âœ… [è°ƒè¯•] JSON è§£ææˆåŠŸ, æå–åˆ° {len(qa_pairs)} ä¸ªé—®ç­”å¯¹")
            
            # 6. éªŒè¯å’Œæ¸…ç†
            print(f"ğŸ§¹ [è°ƒè¯•] éªŒè¯å’Œæ¸…ç†é—®ç­”å¯¹...")
            qa_pairs = self._validate_qa_pairs(qa_pairs, num_tests)
            print(f"âœ… [è°ƒè¯•] æœ€ç»ˆé—®ç­”å¯¹æ•°é‡: {len(qa_pairs)}")
            
            return qa_pairs
        
        except Exception as e:
            print(f"âŒ [è°ƒè¯•] å¼‚å¸¸è¯¦æƒ…:")
            print(f"   å¼‚å¸¸ç±»å‹: {type(e).__name__}")
            print(f"   å¼‚å¸¸ä¿¡æ¯: {e}")
            import traceback
            print(f"   å †æ ˆè·Ÿè¸ª:\n{traceback.format_exc()}")
            print(f"âš ï¸ LLM æå–å¤±è´¥: {e}, ä½¿ç”¨å¯å‘å¼å›é€€")
            return self._heuristic_generate_qa_pairs(num_tests)

    
    async def _load_documents(self, file_paths: List[str]) -> str:
        """åŠ è½½æ–‡æ¡£å†…å®¹
        
        Args:
            file_paths: æ–‡æ¡£è·¯å¾„åˆ—è¡¨
        
        Returns:
            åˆå¹¶çš„æ–‡æ¡£å†…å®¹
        """
        contents = []
        for file_path in file_paths[:5]:  # æœ€å¤šåŠ è½½ 5 ä¸ªæ–‡æ¡£
            try:
                path = Path(file_path)
                if path.exists() and path.suffix in ['.txt', '.md']:
                    content = path.read_text(encoding='utf-8')
                    contents.append(f"## {path.name}\n\n{content}")
            except Exception as e:
                print(f"âš ï¸ æ— æ³•åŠ è½½æ–‡æ¡£ {file_path}: {e}")
        
        return "\n\n".join(contents) if contents else "ç¤ºä¾‹æ–‡æ¡£å†…å®¹"
    
    def _load_prompt_template(self, template_name: str) -> str:
        """åŠ è½½ Prompt æ¨¡æ¿
        
        Args:
            template_name: æ¨¡æ¿æ–‡ä»¶å
        
        Returns:
            æ¨¡æ¿å†…å®¹
        """
        template_path = Path(__file__).parent.parent / "prompts" / template_name
        if template_path.exists():
            return template_path.read_text(encoding='utf-8')
        else:
            return "è¯·ä»æ–‡æ¡£ä¸­æå– {num_tests} ä¸ªé—®ç­”å¯¹:\n\n{document_content}"
    
    def _parse_json_response(self, response: str) -> List[Dict[str, str]]:
        """è§£æ LLM çš„ JSON å“åº” (å¢å¼ºç‰ˆ - ä¸‰å±‚è§£æç­–ç•¥)
        
        Args:
            response: LLM å“åº”
        
        Returns:
            é—®ç­”å¯¹åˆ—è¡¨
        """
        import re
        
        # 1. æå– JSON ä»£ç å—
        json_match = re.search(r'```json\s*(.*?)\s*```', response, re.DOTALL)
        if json_match:
            json_str = json_match.group(1)
        else:
            json_str = response
        
        # 2. Tier 1: å°è¯•ç›´æ¥è§£æ
        try:
            qa_pairs = json.loads(json_str)
            if isinstance(qa_pairs, list):
                print(f"âœ… JSON è§£ææˆåŠŸ (Tier 1: ç›´æ¥è§£æ)")
                return qa_pairs
        except json.JSONDecodeError:
            pass
        
        # 3. Tier 2: å°è¯•ä¿®å¤å¸¸è§æ ¼å¼é”™è¯¯
        try:
            # ä¿®å¤ç¼ºå°‘èŠ±æ‹¬å·çš„é—®é¢˜: [ "question" â†’ [{"question"
            fixed_json = re.sub(
                r'\[\s*"question"',
                r'[{"question"',
                json_str
            )
            # ä¿®å¤å¯¹è±¡é—´ç¼ºå°‘èŠ±æ‹¬å·: }, "question" â†’ },{"question"
            fixed_json = re.sub(
                r'}\s*,\s*"question"',
                r'},{"question"',
                fixed_json
            )
            # ä¿®å¤æ•°ç»„ç»“å°¾ç¼ºå°‘èŠ±æ‹¬å·: "answer": "..." ] â†’ "answer": "..."}]
            fixed_json = re.sub(
                r'"\s*\]$',
                r'"}]',
                fixed_json
            )
            
            qa_pairs = json.loads(fixed_json)
            if isinstance(qa_pairs, list):
                print(f"âœ… JSON è§£ææˆåŠŸ (Tier 2: æ ¼å¼ä¿®å¤)")
                return qa_pairs
        except:
            pass
        
        # 4. Tier 3: ä½¿ç”¨æ­£åˆ™æå–é—®ç­”å¯¹ (æœ€åçš„å…œåº•)
        questions = re.findall(r'"question"\s*:\s*"([^"]+)"', json_str)
        answers = re.findall(r'"expected_answer"\s*:\s*"([^"]+)"', json_str)
        
        if questions and answers and len(questions) == len(answers):
            print(f"âœ… JSON è§£ææˆåŠŸ (Tier 3: æ­£åˆ™æå–, {len(questions)} å¯¹)")
            return [
                {"question": q, "expected_answer": a}
                for q, a in zip(questions, answers)
            ]
        
        # æ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥
        return []
    
    def _validate_qa_pairs(
        self,
        qa_pairs: List[Dict[str, str]],
        num_tests: int
    ) -> List[Dict[str, str]]:
        """éªŒè¯å’Œæ¸…ç†é—®ç­”å¯¹
        
        Args:
            qa_pairs: åŸå§‹é—®ç­”å¯¹
            num_tests: éœ€è¦çš„æ•°é‡
        
        Returns:
            éªŒè¯åçš„é—®ç­”å¯¹
        """
        validated = []
        for qa in qa_pairs:
            if isinstance(qa, dict) and 'question' in qa and 'expected_answer' in qa:
                validated.append({
                    'question': qa['question'].strip(),
                    'expected_answer': qa['expected_answer'].strip()
                })
        
        # å¦‚æœæ•°é‡ä¸è¶³,è¡¥å……ç¤ºä¾‹
        while len(validated) < num_tests:
            validated.append({
                'question': f"ç¤ºä¾‹é—®é¢˜ {len(validated) + 1}",
                'expected_answer': f"ç¤ºä¾‹ç­”æ¡ˆ {len(validated) + 1}"
            })
        
        return validated[:num_tests]
    
    def _heuristic_generate_qa_pairs(self, num_tests: int) -> List[Dict[str, str]]:
        """å¯å‘å¼ç”Ÿæˆé—®ç­”å¯¹ (LLM å¤±è´¥æ—¶çš„å›é€€)
        
        Args:
            num_tests: éœ€è¦çš„æ•°é‡
        
        Returns:
            é—®ç­”å¯¹åˆ—è¡¨
        """
        return [
            {
                "question": f"è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•é—®é¢˜ {i}",
                "expected_answer": f"è¿™æ˜¯å¯¹åº”çš„æµ‹è¯•ç­”æ¡ˆ {i}"
            }
            for i in range(1, num_tests + 1)
        ]
    
    async def _generate_logic_tests(
        self,
        project_meta: ProjectMeta,
        num_tests: int
    ) -> str:
        """ç”Ÿæˆ Logic æµ‹è¯• (G-Eval,éªŒè¯å·¥å…·è°ƒç”¨å’Œæµç¨‹)
        
        Args:
            project_meta: é¡¹ç›®å…ƒä¿¡æ¯
            num_tests: æµ‹è¯•æ•°é‡
        
        Returns:
            Logic æµ‹è¯•å‡½æ•°ä»£ç 
        """
        # åŸºäº task_type ç”Ÿæˆä¸åŒçš„æµ‹è¯•
        if project_meta.task_type in [TaskType.ANALYSIS, TaskType.SEARCH]:
            return self._generate_tool_usage_tests(project_meta, num_tests)
        else:
            return self._generate_basic_logic_tests(project_meta, num_tests)
    
    def _generate_tool_usage_tests(
        self,
        project_meta: ProjectMeta,
        num_tests: int
    ) -> str:
        """ç”Ÿæˆå·¥å…·ä½¿ç”¨æµ‹è¯• (G-Eval)
        
        åŸºäº user_intent åŠ¨æ€ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹ï¼Œè€Œä¸æ˜¯ç¡¬ç¼–ç ã€‚
        """
        # 1. ç¡®å®šæµ‹è¯•æŸ¥è¯¢
        if project_meta.task_type == TaskType.SEARCH:
            # å°è¯•ä» user_intent ä¸­æå–æ›´æœ‰æ„ä¹‰çš„æŸ¥è¯¢ï¼Œæˆ–è€…ä½¿ç”¨é€šç”¨æ¨¡æ¿
            query_prompt = f"Executing task: {project_meta.user_intent_summary[:50]}..."
            criteria = """
            è¯„ä¼°æ ‡å‡†:
            1. Agent å¿…é¡»è°ƒç”¨æœç´¢ç±»å·¥å…· (å¦‚ google_scholar, arxiv, duckduckgo ç­‰)
            2. æœ€ç»ˆå›ç­”å¿…é¡»åŒ…å«ä»å·¥å…·è·å–çš„ä¿¡æ¯
            3. å›ç­”å¿…é¡»ç›´æ¥è§£å†³ç”¨æˆ·çš„éœ€æ±‚
            """
        else:
            query_prompt = "æµ‹è¯•å·¥å…·è°ƒç”¨èƒ½åŠ›"
            criteria = """
            è¯„ä¼°æ ‡å‡†:
            1. Agent å¿…é¡»è°ƒç”¨åˆé€‚çš„å·¥å…·æ¥è§£å†³é—®é¢˜
            2. å·¥å…·è°ƒç”¨å‚æ•°å¿…é¡»æ­£ç¡®
            """
            
        # 2. æ„é€ æµ‹è¯•å‡½æ•°
        # æˆ‘ä»¬ä½¿ç”¨ LLM æ¥ç”Ÿæˆæ›´è‡ªç„¶çš„æŸ¥è¯¢ï¼Œæˆ–è€…ç›´æ¥ä½¿ç”¨ User Intent
        test_query = project_meta.user_intent_summary.replace('"', '\\"')
        
        test_func = f'''
# ==================== Logic æµ‹è¯• - å·¥å…·ä½¿ç”¨ ====================
# éªŒè¯å·¥å…·è°ƒç”¨é€»è¾‘æ˜¯å¦æ­£ç¡®

def test_tool_usage_correctness():
    """æµ‹è¯•: å·¥å…·è°ƒç”¨é€»è¾‘ (Mocked Connection)"""
    query = "{test_query}"
    
    # ğŸ•µï¸â€â™€ï¸ Setup Mocks (æ‹¦æˆªçœŸå®å·¥å…·è°ƒç”¨)
    # åˆ›å»ºçœŸå®çš„ BaseTool å­ç±» (LangChain bind_tools() éœ€è¦)
    from langchain_core.tools import BaseTool
    from pydantic import Field
    
    class MockTavilyTool(BaseTool):
        name: str = "tavily_search"
        description: str = "Mock Tavily search tool for testing"
        
        def _run(self, query: str) -> str:
            return "[Mocked Tool Output] Request processed successfully. Result: 42 (or relevant info)"
        
        async def _arun(self, query: str) -> str:
            return self._run(query)
    
    with patch('agent.tools') as mock_tools:
        # ä½¿ç”¨çœŸå®çš„ BaseTool å­ç±»
        mock_tool = MockTavilyTool()
        
        # æ›¿æ¢ tools åˆ—è¡¨
        mock_tools.__iter__.return_value = [mock_tool]
        mock_tools.__len__.return_value = 1
        
        
        # è¿è¡Œ Agent
        output, trace = run_agent(query, return_trace=True)
        
        # éªŒè¯æ˜¯å¦æœ‰å·¥å…·è¢«è°ƒç”¨ (æ£€æŸ¥ trace ä¸­çš„å·¥å…·è°ƒç”¨è®°å½•)
        tool_called = any(
            step.get("action") == "tool_call" and step.get("tool_name") == "tavily_search"
            for step in trace
        )
        
        
        # æ„é€ æµ‹è¯•ç”¨ä¾‹ä¸Šä¸‹æ–‡
        mock_logs = [f"Mocked Call: tavily_search"] if tool_called else []
        
        test_case = LLMTestCase(
            input=query,
            actual_output=output,
            retrieval_context=[json.dumps(trace, ensure_ascii=False)] + mock_logs
        )
        
        # è‡ªå®šä¹‰ G-Eval æŒ‡æ ‡
        tool_correctness = GEval(
            name="Tool Selection Correctness",
            criteria=\"\"\"{criteria}\"\"\",
            evaluation_params=[
                LLMTestCaseParams.INPUT,
                LLMTestCaseParams.ACTUAL_OUTPUT,
                LLMTestCaseParams.RETRIEVAL_CONTEXT
            ],
            threshold=0.7,
            model=judge_llm
        )
        
        # æ–­è¨€
        assert_test(test_case, [tool_correctness])
        
        
        # æ£€æŸ¥å·¥å…·è°ƒç”¨ (å¦‚æœé¢„æœŸéœ€è¦)
        if "æ— å·¥å…·" not in query and "ä½ å¥½" not in query:
             assert tool_called, f"é¢„æœŸè°ƒç”¨å·¥å…·, å®é™…æœªè°ƒç”¨"
        
        
        print(f"âœ… å·¥å…·æµ‹è¯•é€šè¿‡ (Mocked: tavily_search, Called: {{tool_called}})")
'''
        return test_func
    
    def _generate_basic_logic_tests(
        self,
        project_meta: ProjectMeta,
        num_tests: int
    ) -> str:
        """ç”ŸæˆåŸºç¡€é€»è¾‘æµ‹è¯•"""
        test_func = '''
# ==================== Logic æµ‹è¯• - åŸºç¡€é€»è¾‘ ====================
# éªŒè¯ Agent çš„åŸºæœ¬å“åº”èƒ½åŠ›

def test_basic_response():
    """æµ‹è¯•: åŸºæœ¬å“åº”èƒ½åŠ›"""
    query = "ä½ å¥½"
    
    # è¿è¡Œ Agent
    output, trace = run_agent(query, return_trace=True)
    
    # æ„é€ æµ‹è¯•ç”¨ä¾‹
    test_case = LLMTestCase(
        input=query,
        actual_output=output
    )
    
    # ä½¿ç”¨ Answer Relevancy æŒ‡æ ‡
    relevancy = AnswerRelevancyMetric(
        threshold=0.7,
        model=judge_llm
    )
    
    # æ–­è¨€
    assert_test(test_case, [relevancy])
    
    # åŸºæœ¬æ£€æŸ¥
    assert len(output) > 0, "è¾“å‡ºä¸åº”ä¸ºç©º"
    print("âœ… åŸºç¡€å“åº”æµ‹è¯•é€šè¿‡")
'''
        return test_func


# ==================== è¾…åŠ©å‡½æ•° ====================

def save_test_file(test_content: str, output_path: Path):
    """ä¿å­˜æµ‹è¯•æ–‡ä»¶
    
    Args:
        test_content: æµ‹è¯•ä»£ç å†…å®¹
        output_path: è¾“å‡ºè·¯å¾„
    """
    output_path.parent.mkdir(parents=True, exist_ok=True)
    output_path.write_text(test_content, encoding="utf-8")
    print(f"âœ… æµ‹è¯•æ–‡ä»¶å·²ä¿å­˜: {output_path}")
