import asyncio
import time
from pathlib import Path
from typing import Optional, List, Dict, Any
from datetime import datetime

from ..llm.builder_client import BuilderClient
from ..config.factory_config import AgentFactoryConfig
from ..schemas.project_meta import ProjectMeta
from ..schemas.graph_structure import GraphStructure
from ..schemas.rag_config import RAGConfig
from ..schemas.tools_config import ToolsConfig
from ..schemas.agent_result import AgentResult
from ..schemas.execution_result import ExecutionResult, ExecutionStatus
from ..schemas.judge_result import FixTarget, JudgeResult
from ..schemas.test_report import IterationReport, TestCaseReport

from .progress_callback import ProgressCallback
from .pm import PM
from .graph_designer import GraphDesigner
from .simulator import Simulator
from .compiler import Compiler
from .test_generator import TestGenerator

# Runner is imported on-demand to support hot reload
from .judge import Judge
from .rag_builder import RAGBuilder
from .tool_selector import ToolSelector
from .report_manager import ReportManager
from .report_manager import ReportManager
from ..utils.git_utils import GitUtils
from ..tools.definitions import CURATED_TOOLS


class AgentFactory:
    """Agent å·¥å‚ - ç¼–æ’æ‰€æœ‰ç»„ä»¶ç”Ÿæˆ Agent"""

    def __init__(
        self,
        config: Optional[AgentFactoryConfig] = None,
        callback: Optional[ProgressCallback] = None,
        log_callback: Optional[callable] = None,
    ):
        self.config = config or AgentFactoryConfig.from_env()
        self.callback = callback
        self.log_callback = log_callback  # ğŸ†• Phase 5: UI æ—¥å¿—å›è°ƒ

        # ğŸ†• v8.0: Load Curated Tools into Registry
        # This ensures Interface Guard can validate tool parameters
        from ..tools import get_global_registry

        registry = get_global_registry()
        for tool_def in CURATED_TOOLS:
            registry.register_definition(tool_def)

        # Initialize Core Components
        self.builder_client = BuilderClient.from_env()
        # Note: BuilderClient.from_env() logic might be needed if not default

        self.pm = PM(self.builder_client)
        self.designer = GraphDesigner(self.builder_client)
        self.simulator = Simulator(self.builder_client)
        self.compiler = Compiler(self.config.template_dir)
        self.test_gen = TestGenerator(self.builder_client)
        self.judge = Judge()
        self.rag_builder = RAGBuilder(self.builder_client)
        self.tool_selector = ToolSelector(self.builder_client)

    def set_callback(self, callback: ProgressCallback):
        self.callback = callback

    def _log(self, message: str, level: str = "INFO"):
        """
        ç»Ÿä¸€æ—¥å¿—æ¥å£ - åŒæ—¶æ”¯æŒ CLI å’Œ UI

        Args:
            message: æ—¥å¿—æ¶ˆæ¯
            level: æ—¥å¿—çº§åˆ« (INFO/WARNING/ERROR/SUCCESS/DEBUG)
        """
        # CLI è¾“å‡º
        if self.callback:
            self.callback.on_log(message)

        # UI å›è°ƒ
        if self.log_callback:
            self.log_callback(message, level)

    async def create_agent(
        self,
        user_input: str,
        file_paths: Optional[List[str]] = None,
        output_dir: Optional[Path] = None,
    ) -> AgentResult:
        """
        å…¨æµç¨‹æ„å»º Agent
        """
        start_time = time.time()

        # Initialize Result (Temporary)
        meta = ProjectMeta(
            agent_name="unknown",
            description="",
            user_intent_summary="",
            has_rag=False,
            task_type="chat",
        )
        agent_dir = Path("./temp")  # will be updated
        graph = None

        try:
            # Step 1: PM Analysis
            meta = await self._step_pm_analysis(user_input, file_paths)

            # Update agent_dir based on agent_name
            agent_dir = output_dir or (self.config.output_base_dir / meta.agent_name)

            # Step 2: RAG & Tools
            rag_config, tools_config = await self._step_resources(meta)

            # Design & Review Loop
            sim_result = None
            feedback = None

            for review_round in range(5):  # Limit review rounds
                if review_round == 0:
                    # Initial Design Loop
                    graph, sim_result = await self._design_loop(meta, rag_config, tools_config)
                else:
                    # Refine based on user feedback
                    if self.callback and feedback:
                        self.callback.on_log(
                            f"æ ¹æ®ç”¨æˆ·åé¦ˆè°ƒæ•´è“å›¾ (Round {review_round}): {feedback[:50]}..."
                        )

                        # Use fix_logic with instructions
                        graph = await self.designer.fix_logic(graph, sim_result, feedback)

                        # Re-simulate
                        if self.callback:
                            self.callback.on_log("é‡æ–°è¿è¡Œä»¿çœŸ...")
                        sim_result = await self.simulator.simulate(graph, "Test Input")

                # Step 4: Blueprint Review (Interactive)
                if self.config.interactive and self.callback:
                    approved, feedback = self.callback.on_blueprint_review(graph, sim_result)
                    if approved:
                        break

                    if not feedback:
                        # Rejected without feedback -> Abort
                        if self.callback:
                            self.callback.on_log("ç”¨æˆ·å–æ¶ˆæ„å»º (Review Rejected)")
                        return AgentResult(
                            agent_name=meta.agent_name,
                            agent_dir=agent_dir,
                            project_meta=meta,
                            graph=graph,
                            success=False,
                        )
                else:
                    # No interactivity, assume approved
                    break

            # Step 5: Build & Evolve
            final_result = await self._build_and_evolve_loop(
                meta, graph, rag_config, tools_config, agent_dir
            )

            final_result.total_time = time.time() - start_time

            if self.callback:
                if final_result.success:
                    self.callback.on_log(f"Agent æ„å»ºæˆåŠŸ! ä½äº: {agent_dir}")
                else:
                    self.callback.on_log("Agent æ„å»ºæœªå®Œå…¨é€šè¿‡æµ‹è¯•ã€‚")

            return final_result

        except Exception as e:
            import traceback

            error_msg = f"{str(e)}\n{traceback.format_exc()}"
            if self.callback:
                self.callback.on_step_error("Create Agent", e)

            # Return failed result
            return AgentResult(
                agent_name=meta.agent_name if "meta" in locals() else "unknown",
                agent_dir=agent_dir if "agent_dir" in locals() else Path("./error"),
                project_meta=(
                    meta
                    if "meta" in locals()
                    else ProjectMeta(
                        agent_name="error",
                        description=str(e),
                        user_intent_summary="",
                        has_rag=False,
                        task_type="chat",
                    )
                ),
                graph=graph if "graph" in locals() and graph else None,
                success=False,
                judge_feedback=JudgeResult(
                    error_type="runtime", fix_target="manual", feedback=error_msg, suggestions=[]
                ),
            )

    async def _step_pm_analysis(
        self, user_input: str, file_paths: Optional[List[str]]
    ) -> ProjectMeta:
        """Step 1: PM éœ€æ±‚åˆ†æ"""
        if self.callback:
            self.callback.on_step_start("PM Agent", 1, 5)
            self.callback.on_log(f"åˆ†æç”¨æˆ·éœ€æ±‚: {user_input[:50]}...")

        # Analyze requirements
        from pathlib import Path

        paths = [Path(p) for p in file_paths] if file_paths else []
        # Use clarification loop
        meta = await self.pm.analyze_with_clarification_loop(user_input, file_paths=paths)

        # Handle clarification needed status
        while meta.status == "clarifying" and self.callback:
            # Ask user for clarification
            self.callback.on_clarification_needed(meta.clarification_questions)

            # Since CLI callback doesn't return answers directly (it just prints),
            # we need to implement input collection here or assume callback handles it?
            # Actually, standard callback interface doesn't return value.
            # But we are in an async loop in factory.
            # Let's implementation input collection here via stdin using input() is bad for library code.
            # Ideally callback should return answers. But Callback signature is void or simple.
            # Let's assume we modify callback interface or handle it here if it's CLI.
            # A better approach:
            # In CLI usage, we might not easily inject input back.
            # Let's look at how we can get answers.

            # For now, let's just proceed with simple collected answers from CLI if possible.
            # Since we can't easily change the architecture now, let's input() here if CLI.
            # But self.callback is generic.

            # Let's iterate using a simple blocking input for now, printing via callback.
            answers = {}
            print("\n(è¯·è¾“å…¥å›ç­”ï¼ŒæŒ‰å›è½¦ç¡®è®¤):")
            for q in meta.clarification_questions:
                try:
                    ans = input(f"Q: {q}\nA: ")
                except EOFError:
                    print(f"Q: {q}\nA: (Auto-skipped due to EOF)")
                    ans = "No specific preference"
                answers[q] = ans

            # Refine
            meta = await self.pm.refine_with_clarification(meta, answers)

            # Re-estimate complexity and plan after refinement
            complexity = await self.pm.estimate_complexity(user_input, bool(paths))
            meta.complexity_score = complexity
            if complexity >= 4:
                execution_plan = await self.pm.create_execution_plan(meta)
                meta.execution_plan = execution_plan

            meta.status = "ready"  # Assume ready after one round for now to avoid infinite loops in strict mode

        if self.callback:
            self.callback.on_step_complete("PM Agent", meta)

        return meta

    async def _step_resources(
        self, meta: ProjectMeta
    ) -> tuple[Optional[RAGConfig], Optional[ToolsConfig]]:
        """Step 2: èµ„æºå‡†å¤‡ (RAG & Tools)"""
        if self.callback:
            self.callback.on_step_start("Resource Config", 2, 5)

        rag_config = None
        tools_config = None

        # Parallel execution if possible, for now sequential
        if meta.has_rag:
            if self.callback:
                self.callback.on_log("é…ç½® RAG ç³»ç»Ÿ...")

            # Create a DataProfile on the fly or just rely on file paths
            # RAGBuilder normally needs DataProfile, but specific method signature might vary.
            # RAGBuilder.design_rag_strategy takes DataProfile.
            # We need to profile first if we want to use design_rag_strategy.
            # Or use RAGBuilder.build_config if it existed (my skeleton had it, but file didn't).
            # The file has design_rag_strategy(profile).
            # So we need to profile data first.
            from .profiler import Profiler

            profiler = Profiler()
            if meta.file_paths:
                from pathlib import Path

                paths = [Path(p) for p in meta.file_paths]
                profile = profiler.analyze(paths)
                rag_config = await self.rag_builder.design_rag_strategy(profile)

        # Tools
        if self.callback:
            self.callback.on_log("é€‰æ‹©å·¥å…·...")
        tools_config = await self.tool_selector.select_tools(meta)

        if self.callback:
            self.callback.on_step_complete(
                "Resource Config",
                {
                    "rag": bool(rag_config),
                    "tools": len(tools_config.enabled_tools) if tools_config else 0,
                },
            )

        return rag_config, tools_config

    async def _design_loop(
        self,
        meta: ProjectMeta,
        rag_config: Optional[RAGConfig],
        tools_config: Optional[ToolsConfig],
    ) -> tuple[GraphStructure, Any]:
        """Step 3: è®¾è®¡ä¸ä»¿çœŸé—­ç¯"""
        if self.callback:
            self.callback.on_step_start("Design & Simulation", 3, 5)

        graph = None
        sim_result = None

        for attempt in range(self.config.max_design_retries):
            if attempt == 0:
                # First design
                if self.callback:
                    self.callback.on_log("ç”Ÿæˆåˆå§‹è“å›¾...")
                graph = await self.designer.design_graph(meta, tools_config, rag_config)
            else:
                # Refine design
                if self.callback:
                    self.callback.on_log(f"ä¼˜åŒ–è“å›¾ (ç¬¬ {attempt} æ¬¡è¿­ä»£)...")
                # Using fix_logic with previous simulation result
                graph = await self.designer.fix_logic(graph, sim_result)

            # Simulate
            if self.callback:
                self.callback.on_log("è¿è¡Œæ²™ç›˜æ¨æ¼”...")

            # Create a sample input for simulation
            # Create a sample input for simulation based on task type and tools
            if meta.has_rag:
                sample_input = "Agent Zero æ˜¯ä»€ä¹ˆé¡¹ç›®ï¼Ÿ"  # Trigger RAG keywords
            elif tools_config and tools_config.enabled_tools:
                # Check for search tools specifically
                search_tools = [
                    "tavily_search",
                    "google_search",
                    "bing_search",
                    "duckduckgo_search",
                ]
                if any(t in tools_config.enabled_tools for t in search_tools):
                    sample_input = "æœç´¢ä¸€ä¸‹æœ€æ–°çš„ AI æ–°é—»"
                else:
                    # Generic tool trigger
                    sample_input = f"ä½¿ç”¨ {tools_config.enabled_tools[0]} å·¥å…·è§£å†³ä¸€ä¸ªç®€å•é—®é¢˜"
            elif meta.task_type == "search":
                sample_input = "æœç´¢ä¸€ä¸‹æœ€æ–°çš„ AI æ–°é—»"
            else:
                sample_input = "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"
            sim_result = await self.simulator.simulate(graph, sample_input)

            # Check for critical errors
            if not sim_result.has_errors():
                if self.callback:
                    self.callback.on_log("ä»¿çœŸé€šè¿‡ âœ…")
                break
            else:
                if self.callback:
                    self.callback.on_log(
                        f"ä»¿çœŸå‘ç°é—®é¢˜: {[i.issue_type for i in sim_result.issues]}"
                    )

        if self.callback:
            self.callback.on_step_complete("Design & Simulation", sim_result)

        return graph, sim_result

    async def _build_and_evolve_loop(
        self,
        meta: ProjectMeta,
        graph: GraphStructure,
        rag_config: Optional[RAGConfig],
        tools_config: Optional[ToolsConfig],
        agent_dir: Path,
    ) -> AgentResult:
        """Step 5: æ„å»ºä¸è¿›åŒ–é—­ç¯"""
        if self.callback:
            self.callback.on_step_start("Build & Evolve", 5, 5)

        final_result = AgentResult(
            agent_name=meta.agent_name,
            agent_dir=agent_dir,
            project_meta=meta,
            graph=graph,
            rag_config=rag_config,
            tools_config=tools_config,
        )

        # Initial Compile
        if self.callback:
            self.callback.on_log("ç”Ÿæˆä»£ç ...")
        compile_result = self.compiler.compile(meta, graph, rag_config, tools_config, agent_dir)
        final_result.generated_files = compile_result.generated_files

        if not compile_result.success:
            if self.callback:
                self.callback.on_step_error("Compilation", Exception(compile_result.error_message))
            return final_result

        # Init Git
        if self.config.enable_git:
            git = GitUtils(agent_dir)
            git.init_repo()
            git.commit("Initial generation")

        # ğŸ†• Check API Keys (Before running anything)
        await self._check_and_prompt_keys(agent_dir, tools_config)

        # ğŸ†• Phase 6: Initialize ReportManager
        report_manager = ReportManager(agent_dir)

        # Instantiate Runner for this agent
        # ğŸ”„ å»¶è¿Ÿå¯¼å…¥ Runner,ç¡®ä¿ä½¿ç”¨æœ€æ–°ä»£ç 
        from .runner import Runner

        runner = Runner(agent_dir)

        # Evolution Loop
        for iteration in range(self.config.max_build_retries):
            final_result.iteration_count = iteration

            # 1. Generate Tests
            if self.callback:
                self.callback.on_log(f"ç”Ÿæˆæµ‹è¯• (Iter {iteration})...")

            test_code = await self.test_gen.generate_deepeval_tests(meta, rag_config)

            # Extract test questions for display using simple regex
            import re

            questions = re.findall(r'input="(.*?)",', test_code)
            if self.callback and questions:
                self.callback.on_log(f"å·²ç”Ÿæˆ {len(questions)} ä¸ªæµ‹è¯•ç”¨ä¾‹:")
                for i, q in enumerate(questions[:3], 1):  # Show max 3
                    self.callback.on_log(f"  {i}. {q[:50]}...")

            # Write test file
            test_dir = agent_dir / "tests"
            test_dir.mkdir(exist_ok=True)
            (test_dir / "test_deepeval.py").write_text(test_code, encoding="utf-8")

            # 2. Run Tests
            should_run_tests = False
            if self.callback:
                if self.callback.on_install_request():
                    self.callback.on_log("æ­£åœ¨å®‰è£…ä¾èµ– (è¯·è€å¿ƒç­‰å¾…)...")
                    if runner.setup_environment():
                        self.callback.on_log("ä¾èµ–å®‰è£…å®Œæˆã€‚")

                        # ğŸ”„ é‡æ–°åŠ è½½ Runner æ¨¡å—å¹¶é‡æ–°åˆ›å»ºå®ä¾‹
                        # å› ä¸º install.bat å¯èƒ½ä¿®æ”¹äº†ä»£ç 
                        import importlib
                        import sys

                        if "src.core.runner" in sys.modules:
                            importlib.reload(sys.modules["src.core.runner"])
                        from .runner import Runner

                        runner = Runner(agent_dir)  # é‡æ–°åˆ›å»ºå®ä¾‹
                        self.callback.on_log("âœ… å·²æ›´æ–° Runner æ¨¡å—")

                        should_run_tests = True
                    else:
                        self.callback.on_log("ä¾èµ–å®‰è£…å¤±è´¥ï¼Œè·³è¿‡æµ‹è¯•ã€‚")
                else:
                    self.callback.on_log("è·³è¿‡å®‰è£…ï¼Œä»…ç”Ÿæˆä»£ç ã€‚")

            if should_run_tests:
                if self.callback:
                    self.callback.on_log("æ‰§è¡Œæµ‹è¯•...")
                # Ensure DeepEval is installed/ready is handled by Runner internally or Compiler pre-install
                test_results = runner.run_deepeval_tests()
                final_result.test_results = test_results

                # ğŸ†• Debug: Log test results
                if self.callback:
                    self.callback.on_log(f"ğŸ” [è°ƒè¯•] æµ‹è¯•æ‰§è¡ŒçŠ¶æ€: {test_results.overall_status}")
                    self.callback.on_log(
                        f"ğŸ” [è°ƒè¯•] æµ‹è¯•ç»“æœæ•°é‡: {len(test_results.test_results)}"
                    )
                    if hasattr(test_results, "stderr") and test_results.stderr:
                        stderr_preview = test_results.stderr[:200].replace("\n", " ")
                        self.callback.on_log(f"ğŸ” [è°ƒè¯•] é”™è¯¯è¾“å‡º: {stderr_preview}...")
                    if hasattr(test_results, "stdout") and test_results.stdout:
                        stdout_preview = test_results.stdout[:200].replace("\n", " ")
                        self.callback.on_log(f"ğŸ” [è°ƒè¯•] æ ‡å‡†è¾“å‡º: {stdout_preview}...")
            else:
                # Create a dummy skipped result to avoid attribution errors later
                final_result.test_results = ExecutionResult(
                    overall_status=ExecutionStatus.SKIPPED,
                    test_results=[],
                    stderr="User skipped installation",
                )

            # ğŸ†• Phase 6: Create Iteration Report
            if self.callback:
                self.callback.on_log(f"ğŸ” [è°ƒè¯•] åˆ›å»ºè¿­ä»£æŠ¥å‘Š (è¿­ä»£ {iteration})...")

            iteration_report = self._create_iteration_report(
                iteration=iteration,
                agent_name=meta.agent_name,
                test_results=final_result.test_results,
                graph=graph,
                rag_config=rag_config,
                tools_config=tools_config,
            )

            # ğŸ†• Debug: Log report details
            if self.callback:
                self.callback.on_log(f"ğŸ” [è°ƒè¯•] æŠ¥å‘Šåˆ›å»ºå®Œæˆ:")
                self.callback.on_log(f"   - æ€»æµ‹è¯•æ•°: {iteration_report.total_tests}")
                self.callback.on_log(f"   - é€šè¿‡: {iteration_report.passed_tests}")
                self.callback.on_log(f"   - å¤±è´¥: {iteration_report.failed_tests}")
                self.callback.on_log(f"   - æµ‹è¯•ç”¨ä¾‹æ•°: {len(iteration_report.test_cases)}")

            # ğŸ†• Phase 6: Save Report
            report_manager.save_iteration_report(iteration_report)

            # 3. Judge
            # Only judge if we actually ran tests
            if (
                should_run_tests
                and final_result.test_results.overall_status != ExecutionStatus.SKIPPED
            ):
                judge_result = self.judge.analyze_result(test_results)
                final_result.judge_feedback = judge_result

                # ğŸ†• Phase 6: LLM æ™ºèƒ½åˆ†æ
                analysis = None
                if judge_result.error_type != "none":
                    from .test_analyzer import TestAnalyzer

                    test_analyzer = TestAnalyzer(self.builder_client)

                    current_config = {
                        "graph": graph.model_dump(),
                        "rag": rag_config.model_dump() if rag_config else None,
                        "tools": tools_config.model_dump() if tools_config else None,
                    }

                    try:
                        analysis = await test_analyzer.analyze_test_report(
                            iteration_report, current_config
                        )

                        # æ›´æ–°æŠ¥å‘Šçš„ judge_feedback,æ·»åŠ  AI åˆ†æ
                        enhanced_feedback = (
                            f"{judge_result.feedback}\n\n"
                            f"ğŸ¤– AI åˆ†æ:\n"
                            f"  ä¸»è¦é—®é¢˜: {analysis.primary_issue}\n"
                            f"  æ ¹æœ¬åŸå› : {analysis.root_cause}\n"
                            f"  é¢„è®¡æˆåŠŸç‡: {analysis.estimated_success_rate:.1%}\n"
                        )

                        # å¦‚æœæœ‰ä¿®å¤ç­–ç•¥,ä¹Ÿæ˜¾ç¤º
                        if analysis.fix_strategy:
                            enhanced_feedback += "\nğŸ’¡ ä¿®å¤ç­–ç•¥:\n"
                            for i, step in enumerate(analysis.fix_strategy[:3], 1):  # æœ€å¤šæ˜¾ç¤º3ä¸ª
                                enhanced_feedback += (
                                    f"  {i}. [{step.priority.upper()}] {step.action}\n"
                                    f"     ç›®æ ‡: {step.target}\n"
                                )

                        iteration_report.judge_feedback = enhanced_feedback
                    except Exception as e:
                        if self.callback:
                            self.callback.on_log(f"âš ï¸ LLM åˆ†æå¤±è´¥: {str(e)}")
                        iteration_report.judge_feedback = judge_result.feedback
                else:
                    iteration_report.judge_feedback = judge_result.feedback

                # Update report with judge feedback
                iteration_report.fix_target = (
                    judge_result.fix_target.value if judge_result.fix_target else None
                )
                iteration_report.error_types = self._count_error_types(judge_result)

                # ğŸ†• Phase 6: Git Commit BEFORE user confirmation
                if self.config.enable_git:
                    commit_msg = (
                        f"Iteration {iteration}: Pass rate {iteration_report.pass_rate:.1%}"
                    )
                    git.commit(commit_msg)
                    iteration_report.git_commit_hash = (
                        git.get_current_commit()
                    )  # ä¿®å¤: æ­£ç¡®çš„æ–¹æ³•å
                    iteration_report.git_commit_message = commit_msg
                    # Re-save report with git info
                    report_manager.save_iteration_report(iteration_report)

                # ğŸ†• Phase 6: Display summary
                if self.callback:
                    summary = report_manager.generate_summary(iteration)
                    self.callback.on_log(summary)

                # ğŸ†• Phase 6: Check pass rate threshold
                if iteration_report.pass_rate >= 0.9:  # 90% pass rate
                    if self.callback:
                        self.callback.on_log("âœ… æµ‹è¯•é€šè¿‡ç‡è¾¾æ ‡ (â‰¥90%), åœæ­¢è¿­ä»£")
                    final_result.success = True
                    break

                # ğŸ†• Phase 6: User confirmation checkpoint
                if self.callback and self.config.interactive:
                    result = self.callback.on_iteration_complete(
                        iteration_report, analysis=None  # Phase 2 will add LLM analysis here
                    )

                    # Handle case where callback might return None
                    if result is None:
                        if self.callback:
                            self.callback.on_log("âš ï¸ å›è°ƒè¿”å› None, é»˜è®¤åœæ­¢è¿­ä»£")
                        break

                    continue_iteration, user_feedback = result

                    if not continue_iteration:
                        if self.callback:
                            self.callback.on_log("ç”¨æˆ·é€‰æ‹©åœæ­¢è¿­ä»£")
                        break

                    if user_feedback:
                        # Store user feedback for potential use in fixes
                        judge_result.feedback += f"\n\nç”¨æˆ·åé¦ˆ: {user_feedback}"
                elif self.callback:
                    # Non-interactive mode: just log and continue
                    self.callback.on_log(
                        f"éäº¤äº’æ¨¡å¼: è‡ªåŠ¨ç»§ç»­è¿­ä»£ (è¿­ä»£ {iteration}/{self.config.max_build_retries-1})"
                    )
                # ç¬¬ 544 è¡Œä¹‹å,ç¬¬ 545 è¡Œä¹‹å‰æ’å…¥ä»¥ä¸‹ä»£ç :

                # 4. ğŸ†• Phase 6: æ‰§è¡Œä¿®å¤ç­–ç•¥
                if self.callback:
                    self.callback.on_log(f"ğŸ”§ å¼€å§‹æ‰§è¡Œä¿®å¤ç­–ç•¥...")

                # å¦‚æœæœ‰ LLM åˆ†æç»“æœ,æ‰§è¡Œä¿®å¤ç­–ç•¥
                if analysis and analysis.fix_strategy:
                    for fix_step in analysis.fix_strategy[:3]:  # æœ€å¤šæ‰§è¡Œå‰3ä¸ªä¿®å¤æ­¥éª¤
                        if self.callback:
                            self.callback.on_log(f"  æ‰§è¡Œæ­¥éª¤ {fix_step.step}: {fix_step.action}")

                        try:
                            if fix_step.target == "rag_builder" and rag_config:
                                # RAG é…ç½®ä¼˜åŒ–
                                from .rag_optimizer import RAGOptimizer

                                rag_optimizer = RAGOptimizer(self.builder_client)

                                new_rag_config = await rag_optimizer.optimize_config(
                                    rag_config, analysis, iteration_report
                                )

                                if self.callback:
                                    self.callback.on_log(
                                        f"    âœ… RAG ä¼˜åŒ–: k_retrieval {rag_config.k_retrieval} â†’ {new_rag_config.k_retrieval}"
                                    )

                                rag_config = new_rag_config
                                # é‡æ–°ç¼–è¯‘
                                self.compiler.compile(
                                    meta, graph, rag_config, tools_config, agent_dir
                                )

                            elif fix_step.target == "tool_selector" and tools_config:
                                # Tools ä¼˜åŒ–
                                from .tool_optimizer import ToolOptimizer

                                tool_optimizer = ToolOptimizer(
                                    self.builder_client, self.tool_selector
                                )

                                new_tools_config = await tool_optimizer.optimize_tools(
                                    tools_config, analysis, meta
                                )

                                if self.callback:
                                    self.callback.on_log(
                                        f"    âœ… Tools ä¼˜åŒ–: {tools_config.enabled_tools} â†’ {new_tools_config.enabled_tools}"
                                    )

                                tools_config = new_tools_config
                                # é‡æ–°ç¼–è¯‘
                                self.compiler.compile(
                                    meta, graph, rag_config, tools_config, agent_dir
                                )

                            elif fix_step.target == "graph_designer":
                                # Graph ä¼˜åŒ– + é‡æ–°ä»¿çœŸ
                                from .graph_optimizer import GraphOptimizer

                                graph_optimizer = GraphOptimizer(self.designer, self.simulator)

                                new_graph, sim_result = await graph_optimizer.optimize_graph(
                                    graph, analysis, meta
                                )

                                if self.callback:
                                    sim_status = (
                                        "âœ… é€šè¿‡" if not sim_result.has_errors() else "âš ï¸ ä»æœ‰é—®é¢˜"
                                    )
                                    self.callback.on_log(
                                        f"    âœ… Graph ä¼˜åŒ–å®Œæˆ,ä»¿çœŸç»“æœ: {sim_status}"
                                    )

                                graph = new_graph
                                # é‡æ–°ç¼–è¯‘
                                self.compiler.compile(
                                    meta, graph, rag_config, tools_config, agent_dir
                                )

                            elif fix_step.target == "compiler":
                                # Compiler ä¾èµ–ä¼˜åŒ–
                                from .compiler_optimizer import CompilerOptimizer

                                compiler_optimizer = CompilerOptimizer(self.compiler)

                                error_msg = test_results.stderr or ""
                                success = await compiler_optimizer.optimize_dependencies(
                                    agent_dir, analysis, error_msg
                                )

                                if success and self.callback:
                                    self.callback.on_log(f"    âœ… Compiler ä¼˜åŒ–: å·²æ›´æ–°ä¾èµ–é¡¹")

                        except Exception as e:
                            if self.callback:
                                self.callback.on_log(f"    âš ï¸ ä¿®å¤æ­¥éª¤å¤±è´¥: {str(e)}")

                # Check if should continue iterating
                if judge_result.error_type == "none" or not judge_result.should_retry:
                    final_result.success = True
                    break
            else:
                # If skipped, we consider it a success in terms of generation
                final_result.success = True
                break

            # 4. Fix (only if we ran tests and failed)
            if self.callback:
                self.callback.on_log(
                    f"è‡ªåŠ¨ä¿®å¤: {judge_result.fix_target} - {judge_result.feedback[:50]}..."
                )

            if judge_result.fix_target == FixTarget.COMPILER:
                # Fix runtime errors by patching code
                feedback_str = f"{judge_result.feedback}\nSuggestions: {judge_result.suggestions}"
                await self._apply_compiler_fix(agent_dir / "agent.py", feedback_str)

                # Check requirements if needed
                if "importerror" in judge_result.feedback.lower():
                    # Simple heuristic: notify user or append to requirements.txt
                    # For now, relying on LLM to patch agent.py implies it might try to fix imports in code,
                    # but requirements.txt is separate.
                    # TODO: Handle requirements.txt patching.
                    pass

            elif judge_result.fix_target == FixTarget.GRAPH_DESIGNER:
                # Logic error requires graph change
                # This is expensive, requires re-design
                graph = await self.designer.fix_logic(graph, feedback=judge_result.feedback)
                # Re-compile
                self.compiler.compile(meta, graph, rag_config, tools_config, agent_dir)

        # ğŸ†• Phase 6: Generate final evolution summary
        if self.callback:
            evolution_summary = report_manager.generate_evolution_summary()
            self.callback.on_log(evolution_summary)

        if self.callback:
            self.callback.on_step_complete("Build & Evolve", final_result)

        return final_result

    async def _apply_compiler_fix(self, file_path: Path, feedback: str):
        """Apply a fix to a source file using LLM."""
        if not file_path.exists():
            return

        current_code = file_path.read_text(encoding="utf-8")

        prompt = f"""# Code Repair Task

The following Python code failed to execute.

## Error & Feedback
{feedback}

## Original Code
```python
{current_code}
```

## Requirement
Fix the code errors. Ensure syntax is correct and imports are valid.
Return ONLY the full corrected code content, without markdown code blocks.
"""
        # Call LLM
        fixed_code = await self.builder_client.call(prompt)

        # If result is a string (expected), clean it
        if isinstance(fixed_code, str):
            fixed_code = fixed_code.replace("```python", "").replace("```", "").strip()
            file_path.write_text(fixed_code, encoding="utf-8")

    def _create_iteration_report(
        self,
        iteration: int,
        agent_name: str,
        test_results: ExecutionResult,
        graph: GraphStructure,
        rag_config: Optional[RAGConfig],
        tools_config: Optional[ToolsConfig],
    ) -> IterationReport:
        """åˆ›å»ºè¿­ä»£æŠ¥å‘Š

        Args:
            iteration: è¿­ä»£ç¼–å·
            agent_name: Agentåç§°
            test_results: æµ‹è¯•ç»“æœ
            graph: Graphç»“æ„
            rag_config: RAGé…ç½®
            tools_config: å·¥å…·é…ç½®

        Returns:
            IterationReport
        """
        # ç»Ÿè®¡æµ‹è¯•ç»“æœ
        total_tests = len(test_results.test_results)
        passed_tests = sum(
            1
            for t in test_results.test_results
            if t.status in [ExecutionStatus.PASS, ExecutionStatus.SUCCESS]
        )
        failed_tests = sum(
            1
            for t in test_results.test_results
            if t.status in [ExecutionStatus.FAIL, ExecutionStatus.FAILED]
        )
        skipped_tests = sum(
            1 for t in test_results.test_results if t.status == ExecutionStatus.SKIPPED
        )
        pass_rate = passed_tests / total_tests if total_tests > 0 else 0.0

        # è½¬æ¢æµ‹è¯•ç”¨ä¾‹
        test_cases = []
        for t in test_results.test_results:
            test_cases.append(
                TestCaseReport(
                    test_id=t.test_id,
                    test_name=t.test_id,  # ä½¿ç”¨ test_id ä½œä¸º test_name
                    status=(
                        t.status.value.upper()
                        if hasattr(t.status, "value")
                        else str(t.status).upper()
                    ),
                    metrics={},  # TestResult æ²¡æœ‰ metrics å­—æ®µ
                    actual_output=t.actual_output or "",
                    expected_output="",  # TestResult æ²¡æœ‰ expected_output å­—æ®µ
                    retrieval_context=[],  # TestResult æ²¡æœ‰ retrieval_context å­—æ®µ
                    error_message=t.error_message or "",
                    duration_seconds=t.duration_ms / 1000.0 if t.duration_ms else 0.0,
                )
            )

        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        avg_metrics = {}
        if test_cases:
            metric_names = set()
            for tc in test_cases:
                metric_names.update(tc.metrics.keys())

            for metric_name in metric_names:
                values = [
                    tc.metrics.get(metric_name, 0.0)
                    for tc in test_cases
                    if metric_name in tc.metrics
                ]
                if values:
                    avg_metrics[f"avg_{metric_name}"] = sum(values) / len(values)

        # åˆ›å»ºæŠ¥å‘Š
        return IterationReport(
            iteration_id=iteration,
            timestamp=datetime.now(),
            agent_name=agent_name,
            total_tests=total_tests,
            passed_tests=passed_tests,
            failed_tests=failed_tests,
            skipped_tests=skipped_tests,
            pass_rate=pass_rate,
            test_cases=test_cases,
            error_types={},  # Will be filled by judge
            fix_target=None,  # Will be filled by judge
            judge_feedback="",  # Will be filled by judge
            graph_snapshot=graph.model_dump() if graph else {},
            rag_config_snapshot=rag_config.model_dump() if rag_config else None,
            tools_config_snapshot=tools_config.model_dump() if tools_config else None,
            avg_metrics=avg_metrics,
        )

    def _count_error_types(self, judge_result: JudgeResult) -> Dict[str, int]:
        """ç»Ÿè®¡é”™è¯¯ç±»å‹

        Args:
            judge_result: Judgeåˆ†æç»“æœ

        Returns:
            é”™è¯¯ç±»å‹è®¡æ•°å­—å…¸
        """
        error_counts = {}
        if judge_result.error_type and judge_result.error_type != "none":
            error_counts[judge_result.error_type] = 1
        return error_counts

    async def _check_and_prompt_keys(self, agent_dir: Path, tools_config: Optional[ToolsConfig]):
        """æ£€æŸ¥å¹¶æç¤ºç¼ºå¤±çš„ API Keys"""
        if not tools_config or not tools_config.enabled_tools:
            return

        from dotenv import dotenv_values, set_key

        env_path = agent_dir / ".env"
        if not env_path.exists():
            return

        current_env = dotenv_values(env_path)
        tool_map = {t["id"]: t for t in CURATED_TOOLS}

        for tool_id in tools_config.enabled_tools:
            tool_def = tool_map.get(tool_id)
            if not tool_def or not tool_def.get("requires_api_key"):
                continue

            env_var = tool_def.get("env_var")
            if not env_var:
                continue

            # Check if key is set (skip if already in .env with value)
            if current_env.get(env_var):
                continue

            # Ask LLM for help text
            help_text = ""
            try:
                # Simple prompt for fast response
                prompt = (
                    f"Provide a very short guide (1 sentence + URL) on how to get "
                    f"the API Key ('{env_var}') for the tool '{tool_def['name']}'. "
                    f"Return ONLY the text."
                )
                help_text = await self.builder_client.call(prompt)
                help_text = help_text.strip().replace('"', "")
            except Exception:
                pass

            # Callback to prompt user
            if self.callback:
                key = self.callback.on_api_key_missing(tool_def["name"], env_var, help_text)
                if key:
                    # Save to .env
                    set_key(env_path, env_var, key)
                    current_env[env_var] = key  # Update cache
                    if self.callback:
                        self.callback.on_log(f"âœ… å·²ä¿å­˜ {env_var}")
                    if self.callback:
                        self.callback.on_log(f"âœ… å·²ä¿å­˜ {env_var}")
