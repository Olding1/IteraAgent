# Agent Zero - å®Œæ•´ RAG ç³»ç»Ÿå®æ–½è®¡åˆ’

**ç›®æ ‡**: è¡¥å……å®Œæ•´çš„ RAG (æ£€ç´¢å¢å¼ºç”Ÿæˆ) åŠŸèƒ½,ä½¿ç”Ÿæˆçš„ Agent å…·å¤‡çœŸæ­£çš„å‘é‡æ£€ç´¢èƒ½åŠ›

**å½“å‰çŠ¶æ€**: RAG ç­–ç•¥è®¾è®¡å®Œæˆ,ä½†ç¼ºå°‘æ ¸å¿ƒçš„å‘é‡åŒ–å’Œæ£€ç´¢å®ç°

**é¢„è®¡æ—¶é—´**: 2-3 å‘¨

---

## ğŸ“Š ç°çŠ¶åˆ†æ

### âœ… å·²å®Œæˆ

- RAG Builder: æ™ºèƒ½ç­–ç•¥è®¾è®¡
- Profiler: æ–‡æ¡£ç‰¹å¾åˆ†æ
- RAGConfig Schema: å®Œæ•´çš„é…ç½®å®šä¹‰
- åŸºç¡€æ¨¡æ¿: agent_template.py.j2 æ¡†æ¶

### âŒ ç¼ºå¤±æ ¸å¿ƒåŠŸèƒ½

```
å®Œæ•´ RAG æµç¨‹:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. æ–‡æ¡£åŠ è½½ (Unstructured/PyPDF)           âœ… éƒ¨åˆ†æ”¯æŒ â”‚
â”‚ 2. æ–‡æ¡£åˆ‡åˆ† (Chunking)                     âœ… é…ç½®å®Œæˆ â”‚
â”‚ 3. å‘é‡åŒ– (Embedding)                      âŒ æœªå®ç°   â”‚
â”‚ 4. å‘é‡å­˜å‚¨ (Vector Store)                 âŒ æœªå®ç°   â”‚
â”‚ 5. æŸ¥è¯¢å‘é‡åŒ–                               âŒ æœªå®ç°   â”‚
â”‚ 6. å‘é‡æ£€ç´¢ (Retrieval)                    âŒ æœªå®ç°   â”‚
â”‚ 7. æ··åˆæ£€ç´¢ (Hybrid Search)                âŒ æœªå®ç°   â”‚
â”‚ 8. é‡æ’åº (Rerank)                         âŒ æœªå®ç°   â”‚
â”‚ 9. ä¸Šä¸‹æ–‡æ‹¼æ¥ + LLM ç”Ÿæˆ                    âœ… å·²æœ‰     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ å®æ–½ç›®æ ‡

### é˜¶æ®µ 2.5: RAG æ ¸å¿ƒåŠŸèƒ½è¡¥å…… (Week 5-6)

**ç›®æ ‡**: å®ç°åŸºç¡€ä½†å®Œæ•´çš„ RAG åŠŸèƒ½

**éªŒæ”¶æ ‡å‡†**:
- âœ… ç”Ÿæˆçš„ Agent èƒ½åŠ è½½æ–‡æ¡£å¹¶å‘é‡åŒ–
- âœ… èƒ½å­˜å‚¨å‘é‡åˆ° ChromaDB
- âœ… èƒ½æ ¹æ®ç”¨æˆ·é—®é¢˜è¿›è¡Œå‘é‡æ£€ç´¢
- âœ… èƒ½è¿”å›ç›¸å…³æ–‡æ¡£ç‰‡æ®µ
- âœ… èƒ½åŸºäºæ£€ç´¢ç»“æœç”Ÿæˆç­”æ¡ˆ

---

## ğŸ“‹ è¯¦ç»†å®æ–½è®¡åˆ’

### Week 5: åŸºç¡€ RAG å®ç°

#### Day 1-2: æ›´æ–° Schema å’Œé…ç½®

**ä»»åŠ¡ 1.1: æ‰©å±• RAGConfig Schema**

**æ–‡ä»¶**: `src/schemas/rag_config.py`

**æ–°å¢å­—æ®µ**:
```python
class RAGConfig(BaseModel):
    # ç°æœ‰å­—æ®µ
    splitter: Literal["recursive", "character", "token", "semantic"]
    chunk_size: int
    chunk_overlap: int
    k_retrieval: int
    embedding_model: str
    retriever_type: Literal["basic", "parent_document", "multi_query"]
    reranker_enabled: bool
    
    # æ–°å¢å­—æ®µ
    vector_store: Literal["chroma", "faiss", "pgvector"] = "chroma"
    persist_directory: str = "./chroma_db"
    collection_name: Optional[str] = None
    
    # åµŒå…¥æ¨¡å‹è¯¦ç»†é…ç½®
    embedding_provider: Literal["openai", "huggingface", "ollama"] = "openai"
    embedding_model_name: str = "text-embedding-3-small"
    embedding_dimension: Optional[int] = None
    
    # æ£€ç´¢é…ç½®
    search_type: Literal["similarity", "mmr", "similarity_score_threshold"] = "similarity"
    score_threshold: Optional[float] = None
    fetch_k: int = 20  # MMR ä½¿ç”¨
    lambda_mult: float = 0.5  # MMR å¤šæ ·æ€§å‚æ•°
    
    # æ··åˆæ£€ç´¢
    enable_hybrid_search: bool = False
    bm25_weight: float = 0.5
    vector_weight: float = 0.5
```

**éªŒæ”¶**: Schema éªŒè¯é€šè¿‡,èƒ½æ­£ç¡®åºåˆ—åŒ–/ååºåˆ—åŒ–

---

**ä»»åŠ¡ 1.2: æ›´æ–° requirements.txt ç”Ÿæˆé€»è¾‘**

**æ–‡ä»¶**: `src/core/compiler.py`

**ä¿®æ”¹**: `_generate_requirements()` æ–¹æ³•

**æ–°å¢ä¾èµ–**:
```python
if has_rag:
    requirements.extend([
        "",
        "# RAG dependencies",
        "chromadb>=0.4.22",
        "langchain-community>=0.2.0",
        "langchain-openai>=0.1.0",
        "pypdf>=3.17.0",
        "python-docx>=1.1.0",
        "tiktoken>=0.5.0",  # Token è®¡æ•°
    ])
    
    # æ ¹æ® embedding_provider æ·»åŠ ä¾èµ–
    if rag_config.embedding_provider == "huggingface":
        requirements.append("sentence-transformers>=2.2.0")
    elif rag_config.embedding_provider == "ollama":
        requirements.append("langchain-ollama>=0.1.0")
```

**éªŒæ”¶**: ç”Ÿæˆçš„ requirements.txt åŒ…å«æ‰€æœ‰å¿…è¦ä¾èµ–

---

#### Day 3-4: åˆ›å»º RAG æ¨¡æ¿ç»„ä»¶

**ä»»åŠ¡ 2.1: åˆ›å»ºåµŒå…¥æ¨¡å‹æ¨¡æ¿**

**æ–‡ä»¶**: `src/templates/rag_embedding.py.j2`

**å†…å®¹**:
```jinja2
# åµŒå…¥æ¨¡å‹åˆå§‹åŒ–
{% if rag_config.embedding_provider == "openai" %}
from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(
    model="{{ rag_config.embedding_model_name }}",
    {% if rag_config.embedding_dimension %}
    dimensions={{ rag_config.embedding_dimension }},
    {% endif %}
)

{% elif rag_config.embedding_provider == "huggingface" %}
from langchain_community.embeddings import HuggingFaceEmbeddings

embeddings = HuggingFaceEmbeddings(
    model_name="{{ rag_config.embedding_model_name }}",
    model_kwargs={'device': 'cpu'},  # æˆ– 'cuda'
    encode_kwargs={'normalize_embeddings': True}
)

{% elif rag_config.embedding_provider == "ollama" %}
from langchain_ollama import OllamaEmbeddings

embeddings = OllamaEmbeddings(
    model="{{ rag_config.embedding_model_name }}",
)

{% endif %}
```

**éªŒæ”¶**: æ¨¡æ¿èƒ½æ­£ç¡®æ¸²æŸ“ä¸åŒçš„åµŒå…¥æ¨¡å‹é…ç½®

---

**ä»»åŠ¡ 2.2: åˆ›å»ºå‘é‡å­˜å‚¨æ¨¡æ¿**

**æ–‡ä»¶**: `src/templates/rag_vectorstore.py.j2`

**å†…å®¹**:
```jinja2
# å‘é‡æ•°æ®åº“åˆå§‹åŒ–
{% if rag_config.vector_store == "chroma" %}
from langchain_community.vectorstores import Chroma

vectorstore = Chroma(
    collection_name="{{ rag_config.collection_name or agent_name + '_docs' }}",
    embedding_function=embeddings,
    persist_directory="{{ rag_config.persist_directory }}"
)

{% elif rag_config.vector_store == "faiss" %}
from langchain_community.vectorstores import FAISS

# FAISS éœ€è¦å…ˆåŠ è½½æ–‡æ¡£ååˆ›å»º
vectorstore = None  # å°†åœ¨æ–‡æ¡£åŠ è½½ååˆå§‹åŒ–

{% elif rag_config.vector_store == "pgvector" %}
from langchain_community.vectorstores import PGVector

CONNECTION_STRING = os.getenv("PGVECTOR_CONNECTION_STRING")
vectorstore = PGVector(
    collection_name="{{ rag_config.collection_name or agent_name + '_docs' }}",
    connection_string=CONNECTION_STRING,
    embedding_function=embeddings,
)

{% endif %}
```

**éªŒæ”¶**: æ¨¡æ¿èƒ½æ­£ç¡®æ¸²æŸ“ä¸åŒçš„å‘é‡æ•°æ®åº“é…ç½®

---

**ä»»åŠ¡ 2.3: åˆ›å»ºæ–‡æ¡£åŠ è½½æ¨¡æ¿**

**æ–‡ä»¶**: `src/templates/rag_document_loader.py.j2`

**å†…å®¹**:
```jinja2
# æ–‡æ¡£åŠ è½½å’Œå¤„ç†
from langchain_community.document_loaders import (
    PyPDFLoader,
    Docx2txtLoader,
    TextLoader,
    UnstructuredMarkdownLoader,
)
from langchain.text_splitter import (
    RecursiveCharacterTextSplitter,
    CharacterTextSplitter,
    TokenTextSplitter,
)
from pathlib import Path

def load_documents(file_paths: list[str]) -> list:
    """åŠ è½½å¤šä¸ªæ–‡æ¡£"""
    documents = []
    
    for file_path in file_paths:
        file_path = Path(file_path)
        
        if not file_path.exists():
            print(f"Warning: File not found: {file_path}")
            continue
        
        # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©åŠ è½½å™¨
        if file_path.suffix.lower() == '.pdf':
            loader = PyPDFLoader(str(file_path))
        elif file_path.suffix.lower() in ['.docx', '.doc']:
            loader = Docx2txtLoader(str(file_path))
        elif file_path.suffix.lower() == '.md':
            loader = UnstructuredMarkdownLoader(str(file_path))
        elif file_path.suffix.lower() == '.txt':
            loader = TextLoader(str(file_path))
        else:
            print(f"Warning: Unsupported file type: {file_path.suffix}")
            continue
        
        try:
            docs = loader.load()
            documents.extend(docs)
            print(f"Loaded {len(docs)} documents from {file_path.name}")
        except Exception as e:
            print(f"Error loading {file_path}: {e}")
    
    return documents

def split_documents(documents: list) -> list:
    """åˆ‡åˆ†æ–‡æ¡£"""
    {% if rag_config.splitter == "recursive" %}
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size={{ rag_config.chunk_size }},
        chunk_overlap={{ rag_config.chunk_overlap }},
        length_function=len,
    )
    {% elif rag_config.splitter == "character" %}
    text_splitter = CharacterTextSplitter(
        chunk_size={{ rag_config.chunk_size }},
        chunk_overlap={{ rag_config.chunk_overlap }},
    )
    {% elif rag_config.splitter == "token" %}
    text_splitter = TokenTextSplitter(
        chunk_size={{ rag_config.chunk_size }},
        chunk_overlap={{ rag_config.chunk_overlap }},
    )
    {% endif %}
    
    splits = text_splitter.split_documents(documents)
    print(f"Split into {len(splits)} chunks")
    return splits

# åŠ è½½å’Œå¤„ç†æ–‡æ¡£
{% if file_paths %}
print("Loading documents...")
documents = load_documents({{ file_paths }})

print("Splitting documents...")
splits = split_documents(documents)

print("Creating vector store...")
{% if rag_config.vector_store == "faiss" %}
# FAISS éœ€è¦ä»æ–‡æ¡£åˆ›å»º
from langchain_community.vectorstores import FAISS
vectorstore = FAISS.from_documents(splits, embeddings)
vectorstore.save_local("{{ rag_config.persist_directory }}")
{% else %}
# ChromaDB/PGVector ç›´æ¥æ·»åŠ 
vectorstore.add_documents(splits)
{% endif %}

print(f"Indexed {len(splits)} document chunks")
{% endif %}
```

**éªŒæ”¶**: èƒ½æ­£ç¡®åŠ è½½ä¸åŒæ ¼å¼çš„æ–‡æ¡£å¹¶åˆ‡åˆ†

---

**ä»»åŠ¡ 2.4: åˆ›å»ºæ£€ç´¢å™¨æ¨¡æ¿**

**æ–‡ä»¶**: `src/templates/rag_retriever.py.j2`

**å†…å®¹**:
```jinja2
# æ£€ç´¢å™¨é…ç½®
{% if rag_config.retriever_type == "basic" %}
# åŸºç¡€æ£€ç´¢å™¨
retriever = vectorstore.as_retriever(
    search_type="{{ rag_config.search_type }}",
    search_kwargs={
        "k": {{ rag_config.k_retrieval }},
        {% if rag_config.search_type == "similarity_score_threshold" %}
        "score_threshold": {{ rag_config.score_threshold or 0.5 }},
        {% elif rag_config.search_type == "mmr" %}
        "fetch_k": {{ rag_config.fetch_k }},
        "lambda_mult": {{ rag_config.lambda_mult }},
        {% endif %}
    }
)

{% elif rag_config.retriever_type == "parent_document" %}
# çˆ¶æ–‡æ¡£æ£€ç´¢å™¨
from langchain.retrievers import ParentDocumentRetriever
from langchain.storage import InMemoryStore
from langchain.text_splitter import RecursiveCharacterTextSplitter

# çˆ¶æ–‡æ¡£å­˜å‚¨
parent_store = InMemoryStore()

# å­æ–‡æ¡£åˆ†å‰²å™¨ (æ›´å°çš„å—ç”¨äºæ£€ç´¢)
child_splitter = RecursiveCharacterTextSplitter(
    chunk_size={{ rag_config.chunk_size // 2 }},
    chunk_overlap={{ rag_config.chunk_overlap // 2 }},
)

retriever = ParentDocumentRetriever(
    vectorstore=vectorstore,
    docstore=parent_store,
    child_splitter=child_splitter,
    search_kwargs={"k": {{ rag_config.k_retrieval }}},
)

{% elif rag_config.retriever_type == "multi_query" %}
# å¤šæŸ¥è¯¢æ£€ç´¢å™¨
from langchain.retrievers.multi_query import MultiQueryRetriever

base_retriever = vectorstore.as_retriever(
    search_kwargs={"k": {{ rag_config.k_retrieval }}}
)

retriever = MultiQueryRetriever.from_llm(
    retriever=base_retriever,
    llm=llm,
)

{% endif %}

{% if rag_config.enable_hybrid_search %}
# æ··åˆæ£€ç´¢ (å‘é‡ + BM25)
from langchain.retrievers import EnsembleRetriever
from langchain_community.retrievers import BM25Retriever

# BM25 æ£€ç´¢å™¨
bm25_retriever = BM25Retriever.from_documents(splits)
bm25_retriever.k = {{ rag_config.k_retrieval }}

# ç»„åˆæ£€ç´¢å™¨
retriever = EnsembleRetriever(
    retrievers=[retriever, bm25_retriever],
    weights=[{{ rag_config.vector_weight }}, {{ rag_config.bm25_weight }}]
)
{% endif %}

{% if rag_config.reranker_enabled %}
# é‡æ’åº
from langchain.retrievers import ContextualCompressionRetriever
from langchain_community.document_compressors import CohereRerank

compressor = CohereRerank(
    model="rerank-english-v2.0",
    top_n={{ rag_config.k_retrieval }}
)

retriever = ContextualCompressionRetriever(
    base_compressor=compressor,
    base_retriever=retriever
)
{% endif %}
```

**éªŒæ”¶**: èƒ½æ­£ç¡®é…ç½®ä¸åŒç±»å‹çš„æ£€ç´¢å™¨

---

**ä»»åŠ¡ 2.5: åˆ›å»º RAG Chain æ¨¡æ¿**

**æ–‡ä»¶**: `src/templates/rag_chain.py.j2`

**å†…å®¹**:
```jinja2
# RAG Chain
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# RAG Prompt
rag_prompt_template = """ä½¿ç”¨ä»¥ä¸‹ä¸Šä¸‹æ–‡æ¥å›ç­”é—®é¢˜ã€‚å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆ,å°±è¯´ä¸çŸ¥é“,ä¸è¦è¯•å›¾ç¼–é€ ç­”æ¡ˆã€‚

ä¸Šä¸‹æ–‡:
{context}

é—®é¢˜: {question}

å›ç­”:"""

RAG_PROMPT = PromptTemplate(
    template=rag_prompt_template,
    input_variables=["context", "question"]
)

# åˆ›å»º RAG Chain
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",  # æˆ– "map_reduce", "refine"
    retriever=retriever,
    return_source_documents=True,
    chain_type_kwargs={"prompt": RAG_PROMPT}
)

def ask_question(question: str) -> dict:
    """ä½¿ç”¨ RAG å›ç­”é—®é¢˜"""
    result = qa_chain({"query": question})
    
    return {
        "answer": result["result"],
        "sources": [
            {
                "content": doc.page_content,
                "metadata": doc.metadata
            }
            for doc in result["source_documents"]
        ]
    }
```

**éªŒæ”¶**: RAG Chain èƒ½æ­£ç¡®æ‰§è¡Œæ£€ç´¢å’Œç”Ÿæˆ

---

#### Day 5: é›†æˆåˆ°ä¸»æ¨¡æ¿

**ä»»åŠ¡ 3.1: æ›´æ–° agent_template.py.j2**

**æ–‡ä»¶**: `src/templates/agent_template.py.j2`

**ä¿®æ”¹**: åœ¨ä¸»æ¨¡æ¿ä¸­é›†æˆæ‰€æœ‰ RAG ç»„ä»¶

**ç»“æ„**:
```jinja2
#!/usr/bin/env python3
"""
{{ agent_name }} - Generated by Agent Zero
Description: {{ description }}
Generated at: {{ timestamp }}
"""

import os
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# LLM åˆå§‹åŒ–
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(
    model=os.getenv("RUNTIME_MODEL", "gpt-3.5-turbo"),
    temperature=float(os.getenv("TEMPERATURE", "0.7")),
)

{% if has_rag %}
# ============================================================================
# RAG ç»„ä»¶
# ============================================================================

{% include 'rag_embedding.py.j2' %}

{% include 'rag_vectorstore.py.j2' %}

{% include 'rag_document_loader.py.j2' %}

{% include 'rag_retriever.py.j2' %}

{% include 'rag_chain.py.j2' %}

{% endif %}

# ============================================================================
# ä¸»ç¨‹åº
# ============================================================================

def main():
    print("=" * 60)
    print(f"ğŸ¤– {{ agent_name }}")
    print("=" * 60)
    print()
    
    {% if has_rag %}
    print("RAG æ¨¡å¼å·²å¯ç”¨")
    print(f"å‘é‡æ•°æ®åº“: {{ rag_config.vector_store }}")
    print(f"åµŒå…¥æ¨¡å‹: {{ rag_config.embedding_model_name }}")
    print(f"æ£€ç´¢å™¨ç±»å‹: {{ rag_config.retriever_type }}")
    print()
    {% endif %}
    
    while True:
        user_input = input("You: ").strip()
        
        if user_input.lower() in ['quit', 'exit', 'q']:
            print("Goodbye!")
            break
        
        if not user_input:
            continue
        
        try:
            {% if has_rag %}
            # ä½¿ç”¨ RAG
            result = ask_question(user_input)
            print(f"\nAgent: {result['answer']}\n")
            
            # æ˜¾ç¤ºæ¥æº
            if result['sources']:
                print("ğŸ“š æ¥æº:")
                for i, source in enumerate(result['sources'][:3], 1):
                    print(f"{i}. {source['content'][:100]}...")
                print()
            {% else %}
            # ç›´æ¥ä½¿ç”¨ LLM
            response = llm.invoke(user_input)
            print(f"\nAgent: {response.content}\n")
            {% endif %}
            
        except Exception as e:
            print(f"\nâŒ Error: {e}\n")

if __name__ == "__main__":
    main()
```

**éªŒæ”¶**: ç”Ÿæˆçš„ agent.py åŒ…å«å®Œæ•´çš„ RAG åŠŸèƒ½

---

#### Day 6-7: æµ‹è¯•å’Œä¼˜åŒ–

**ä»»åŠ¡ 4.1: åˆ›å»º RAG å•å…ƒæµ‹è¯•**

**æ–‡ä»¶**: `tests/unit/test_rag_components.py`

**æµ‹è¯•å†…å®¹**:
```python
import pytest
from pathlib import Path
import tempfile

def test_rag_config_validation():
    """æµ‹è¯• RAGConfig éªŒè¯"""
    from src.schemas import RAGConfig
    
    config = RAGConfig(
        splitter="recursive",
        chunk_size=1000,
        chunk_overlap=200,
        k_retrieval=5,
        embedding_model="openai",
        retriever_type="basic",
        reranker_enabled=False,
    )
    
    assert config.chunk_size == 1000
    assert config.splitter == "recursive"

def test_embedding_template_rendering():
    """æµ‹è¯•åµŒå…¥æ¨¡å‹æ¨¡æ¿æ¸²æŸ“"""
    from jinja2 import Environment, FileSystemLoader
    
    env = Environment(loader=FileSystemLoader("src/templates"))
    template = env.get_template("rag_embedding.py.j2")
    
    # æµ‹è¯• OpenAI
    result = template.render(
        rag_config={
            "embedding_provider": "openai",
            "embedding_model_name": "text-embedding-3-small"
        }
    )
    
    assert "OpenAIEmbeddings" in result
    assert "text-embedding-3-small" in result

# æ›´å¤šæµ‹è¯•...
```

**éªŒæ”¶**: æ‰€æœ‰å•å…ƒæµ‹è¯•é€šè¿‡

---

**ä»»åŠ¡ 4.2: åˆ›å»º RAG E2E æµ‹è¯•**

**æ–‡ä»¶**: `tests/e2e/test_rag_full_pipeline.py`

**æµ‹è¯•æµç¨‹**:
```python
async def test_rag_full_pipeline():
    """æµ‹è¯•å®Œæ•´çš„ RAG æµç¨‹"""
    
    # 1. åˆ›å»ºæµ‹è¯•æ–‡æ¡£
    with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:
        f.write("Agent Zero æ˜¯ä¸€ä¸ªæ™ºèƒ½ä½“æ„å»ºå·¥å‚ã€‚å®ƒå¯ä»¥è‡ªåŠ¨ç”Ÿæˆ Agentã€‚")
        test_file = Path(f.name)
    
    # 2. PM åˆ†æéœ€æ±‚
    pm = PM(builder_client)
    project_meta = await pm.analyze_requirements(
        "åˆ›å»ºä¸€ä¸ªèƒ½å›ç­” Agent Zero ç›¸å…³é—®é¢˜çš„åŠ©æ‰‹",
        file_paths=[test_file]
    )
    
    assert project_meta.has_rag is True
    
    # 3. Profiler åˆ†ææ–‡æ¡£
    profiler = Profiler()
    data_profile = profiler.analyze([test_file])
    
    # 4. RAG Builder è®¾è®¡ç­–ç•¥
    rag_builder = RAGBuilder(builder_client)
    rag_config = await rag_builder.design_rag_strategy(data_profile)
    
    # 5. ç¼–è¯‘ç”Ÿæˆ Agent
    compiler = Compiler(template_dir=Path("src/templates"))
    output_dir = Path("agents/rag_test")
    
    result = compiler.compile(
        project_meta=project_meta,
        graph=graph_structure,
        rag_config=rag_config,
        tools_config=ToolsConfig(enabled_tools=[]),
        output_dir=output_dir
    )
    
    assert result.success
    assert (output_dir / "agent.py").exists()
    
    # 6. éªŒè¯ç”Ÿæˆçš„ä»£ç åŒ…å« RAG ç»„ä»¶
    agent_code = (output_dir / "agent.py").read_text()
    assert "embeddings" in agent_code
    assert "vectorstore" in agent_code
    assert "retriever" in agent_code
    assert "qa_chain" in agent_code
    
    # 7. è®¾ç½®ç¯å¢ƒ
    env_manager = EnvManager(output_dir)
    await env_manager.setup_environment()
    
    # 8. è¿è¡Œ Agent å¹¶æµ‹è¯•
    # (éœ€è¦å®é™…è¿è¡Œç”Ÿæˆçš„ agent.py)
    
    print("âœ… RAG Full Pipeline Test PASSED!")
```

**éªŒæ”¶**: E2E æµ‹è¯•é€šè¿‡,ç”Ÿæˆçš„ Agent èƒ½æ­£ç¡®å›ç­”é—®é¢˜

---

### Week 6: é«˜çº§ RAG åŠŸèƒ½

#### Day 8-9: æ··åˆæ£€ç´¢å®ç°

**ä»»åŠ¡ 5.1: å®ç° BM25 æ£€ç´¢å™¨**

**æ–‡ä»¶**: `src/templates/rag_retriever.py.j2` (æ‰©å±•)

**æ–°å¢å†…å®¹**:
```jinja2
{% if rag_config.enable_hybrid_search %}
# BM25 æ£€ç´¢å™¨ (å…³é”®è¯æ£€ç´¢)
from langchain_community.retrievers import BM25Retriever
from rank_bm25 import BM25Okapi
import jieba  # ä¸­æ–‡åˆ†è¯

def tokenize_chinese(text: str) -> list[str]:
    """ä¸­æ–‡åˆ†è¯"""
    return list(jieba.cut(text))

# åˆ›å»º BM25 æ£€ç´¢å™¨
bm25_retriever = BM25Retriever.from_documents(
    splits,
    preprocess_func=tokenize_chinese if "{{ language }}" == "zh-CN" else None
)
bm25_retriever.k = {{ rag_config.k_retrieval }}

# ç»„åˆæ£€ç´¢å™¨ (RRF èåˆ)
from langchain.retrievers import EnsembleRetriever

ensemble_retriever = EnsembleRetriever(
    retrievers=[vector_retriever, bm25_retriever],
    weights=[{{ rag_config.vector_weight }}, {{ rag_config.bm25_weight }}]
)

retriever = ensemble_retriever
{% endif %}
```

**æ–°å¢ä¾èµ–**:
```
rank-bm25>=0.2.2
jieba>=0.42.1  # ä¸­æ–‡åˆ†è¯
```

**éªŒæ”¶**: æ··åˆæ£€ç´¢èƒ½æé«˜å‡†ç¡®ç‡

---

**ä»»åŠ¡ 5.2: å®ç°é‡æ’åº**

**æ–‡ä»¶**: `src/templates/rag_retriever.py.j2` (æ‰©å±•)

**æ”¯æŒå¤šç§ Reranker**:
```jinja2
{% if rag_config.reranker_enabled %}
from langchain.retrievers import ContextualCompressionRetriever

{% if rag_config.reranker_provider == "cohere" %}
# Cohere Rerank
from langchain_community.document_compressors import CohereRerank

compressor = CohereRerank(
    model="rerank-english-v2.0",
    top_n={{ rag_config.k_retrieval }}
)

{% elif rag_config.reranker_provider == "bge" %}
# BGE Reranker (æœ¬åœ°)
from langchain_community.document_compressors import HuggingFaceBgeRerank

compressor = HuggingFaceBgeRerank(
    model_name="BAAI/bge-reranker-v2-m3",
    top_n={{ rag_config.k_retrieval }}
)

{% endif %}

retriever = ContextualCompressionRetriever(
    base_compressor=compressor,
    base_retriever=retriever
)
{% endif %}
```

**éªŒæ”¶**: Rerank èƒ½æé«˜ Top-K ç»“æœçš„ç›¸å…³æ€§

---

#### Day 10-11: å¤šå‘é‡æ•°æ®åº“æ”¯æŒ

**ä»»åŠ¡ 6.1: æ”¯æŒ Qdrant**

**æ–‡ä»¶**: `src/templates/rag_vectorstore.py.j2` (æ‰©å±•)

**æ–°å¢å†…å®¹**:
```jinja2
{% elif rag_config.vector_store == "qdrant" %}
from langchain_community.vectorstores import Qdrant
from qdrant_client import QdrantClient

# Qdrant å®¢æˆ·ç«¯
qdrant_client = QdrantClient(
    url=os.getenv("QDRANT_URL", "http://localhost:6333"),
    api_key=os.getenv("QDRANT_API_KEY"),
)

vectorstore = Qdrant(
    client=qdrant_client,
    collection_name="{{ rag_config.collection_name or agent_name + '_docs' }}",
    embeddings=embeddings,
)
{% endif %}
```

**éªŒæ”¶**: èƒ½ä½¿ç”¨ Qdrant ä½œä¸ºå‘é‡æ•°æ®åº“

---

**ä»»åŠ¡ 6.2: æ”¯æŒ Milvus**

**æ–°å¢å†…å®¹**:
```jinja2
{% elif rag_config.vector_store == "milvus" %}
from langchain_community.vectorstores import Milvus

vectorstore = Milvus(
    embedding_function=embeddings,
    collection_name="{{ rag_config.collection_name or agent_name + '_docs' }}",
    connection_args={
        "host": os.getenv("MILVUS_HOST", "localhost"),
        "port": os.getenv("MILVUS_PORT", "19530"),
    },
)
{% endif %}
```

**éªŒæ”¶**: èƒ½ä½¿ç”¨ Milvus ä½œä¸ºå‘é‡æ•°æ®åº“

---

#### Day 12-13: æ–‡æ¡£å’Œç¤ºä¾‹

**ä»»åŠ¡ 7.1: æ›´æ–°ç”¨æˆ·æŒ‡å—**

**æ–‡ä»¶**: `docs/RAG_GUIDE.md`

**å†…å®¹**:
```markdown
# Agent Zero RAG ä½¿ç”¨æŒ‡å—

## ä»€ä¹ˆæ˜¯ RAG

RAG (Retrieval-Augmented Generation) æ˜¯ä¸€ç§ç»“åˆæ£€ç´¢å’Œç”Ÿæˆçš„æŠ€æœ¯...

## å¦‚ä½•ä½¿ç”¨

### 1. å‡†å¤‡æ–‡æ¡£
æ”¯æŒçš„æ ¼å¼: PDF, DOCX, TXT, MD

### 2. åˆ›å»º RAG Agent
```bash
python start.py
# é€‰æ‹©åˆ›å»º Agent
# ä¸Šä¼ æ–‡æ¡£
# ç³»ç»Ÿè‡ªåŠ¨é…ç½® RAG
```

### 3. é…ç½®é€‰é¡¹
- åµŒå…¥æ¨¡å‹: OpenAI, BGE-M3, Ollama
- å‘é‡æ•°æ®åº“: ChromaDB, Qdrant, Milvus
- æ£€ç´¢å™¨: åŸºç¡€, çˆ¶æ–‡æ¡£, å¤šæŸ¥è¯¢
- æ··åˆæ£€ç´¢: å‘é‡ + BM25
- é‡æ’åº: Cohere, BGE

## æœ€ä½³å®è·µ
...
```

**éªŒæ”¶**: æ–‡æ¡£æ¸…æ™°æ˜“æ‡‚

---

**ä»»åŠ¡ 7.2: åˆ›å»ºç¤ºä¾‹ Agent**

**æ–‡ä»¶**: `examples/rag_agent_example.py`

**å†…å®¹**: å®Œæ•´çš„ RAG Agent ä½¿ç”¨ç¤ºä¾‹

**éªŒæ”¶**: ç¤ºä¾‹èƒ½æ­£å¸¸è¿è¡Œ

---

#### Day 14: æ€§èƒ½ä¼˜åŒ–å’Œæµ‹è¯•

**ä»»åŠ¡ 8.1: æ€§èƒ½ä¼˜åŒ–**

**ä¼˜åŒ–ç‚¹**:
1. å‘é‡åŒ–æ‰¹å¤„ç†
2. æ£€ç´¢ç»“æœç¼“å­˜
3. å»¶è¿ŸåŠ è½½å‘é‡æ•°æ®åº“
4. å¼‚æ­¥æ£€ç´¢

**éªŒæ”¶**: å“åº”æ—¶é—´ < 2 ç§’

---

**ä»»åŠ¡ 8.2: å…¨é¢æµ‹è¯•**

**æµ‹è¯•åœºæ™¯**:
1. å•æ–‡æ¡£ RAG
2. å¤šæ–‡æ¡£ RAG
3. å¤§æ–‡æ¡£ RAG (>100 é¡µ)
4. æ··åˆæ£€ç´¢
5. é‡æ’åº
6. ä¸åŒåµŒå…¥æ¨¡å‹
7. ä¸åŒå‘é‡æ•°æ®åº“

**éªŒæ”¶**: æ‰€æœ‰åœºæ™¯æµ‹è¯•é€šè¿‡

---

## ğŸ“Š éªŒæ”¶æ ‡å‡†

### åŠŸèƒ½éªŒæ”¶

- [ ] èƒ½åŠ è½½ PDF/DOCX/TXT/MD æ–‡æ¡£
- [ ] èƒ½æ­£ç¡®åˆ‡åˆ†æ–‡æ¡£
- [ ] èƒ½å‘é‡åŒ–æ–‡æ¡£å¹¶å­˜å‚¨
- [ ] èƒ½æ ¹æ®é—®é¢˜æ£€ç´¢ç›¸å…³æ–‡æ¡£
- [ ] èƒ½åŸºäºæ£€ç´¢ç»“æœç”Ÿæˆç­”æ¡ˆ
- [ ] æ”¯æŒè‡³å°‘ 2 ç§åµŒå…¥æ¨¡å‹
- [ ] æ”¯æŒè‡³å°‘ 2 ç§å‘é‡æ•°æ®åº“
- [ ] æ”¯æŒæ··åˆæ£€ç´¢ (å¯é€‰)
- [ ] æ”¯æŒé‡æ’åº (å¯é€‰)

### æ€§èƒ½éªŒæ”¶

- [ ] æ–‡æ¡£ç´¢å¼•æ—¶é—´ < 1 åˆ†é’Ÿ (100 é¡µ)
- [ ] æ£€ç´¢å“åº”æ—¶é—´ < 2 ç§’
- [ ] å‡†ç¡®ç‡ > 80% (åŸºå‡†æµ‹è¯•é›†)

### ä»£ç è´¨é‡

- [ ] æ‰€æœ‰æ¨¡æ¿èƒ½æ­£ç¡®æ¸²æŸ“
- [ ] ç”Ÿæˆçš„ä»£ç ç¬¦åˆ PEP 8
- [ ] å•å…ƒæµ‹è¯•è¦†ç›–ç‡ > 70%
- [ ] E2E æµ‹è¯•é€šè¿‡
- [ ] æ–‡æ¡£å®Œæ•´

---

## ğŸ”„ åç»­ä¼˜åŒ– (Week 7+)

### é«˜çº§åŠŸèƒ½

1. **GraphRAG**
   - å®ä½“æå–
   - å…³ç³»æ„å»º
   - å›¾è°±æ£€ç´¢

2. **Agentic RAG**
   - Self-RAG (è‡ªæˆ‘åæ€)
   - Adaptive RAG (è‡ªé€‚åº”æ£€ç´¢)
   - æŸ¥è¯¢é‡å†™

3. **å¤šæ¨¡æ€ RAG**
   - å›¾ç‰‡æ£€ç´¢
   - è¡¨æ ¼ç†è§£
   - OCR é›†æˆ

4. **å¢é‡æ›´æ–°**
   - æ–‡æ¡£å¢é‡ç´¢å¼•
   - å‘é‡æ•°æ®åº“æ›´æ–°
   - ç‰ˆæœ¬ç®¡ç†

### æ€§èƒ½ä¼˜åŒ–

1. **ç¼“å­˜æœºåˆ¶**
   - æŸ¥è¯¢ç¼“å­˜
   - åµŒå…¥ç¼“å­˜
   - ç»“æœç¼“å­˜

2. **å¹¶è¡Œå¤„ç†**
   - æ‰¹é‡å‘é‡åŒ–
   - å¹¶è¡Œæ£€ç´¢
   - å¼‚æ­¥å¤„ç†

3. **èµ„æºä¼˜åŒ–**
   - å†…å­˜ç®¡ç†
   - GPU åŠ é€Ÿ
   - æ¨¡å‹é‡åŒ–

---

## ğŸ“ˆ é‡Œç¨‹ç¢‘

### Week 5 ç»“æŸ
- âœ… åŸºç¡€ RAG åŠŸèƒ½å®Œæˆ
- âœ… æ”¯æŒ ChromaDB + OpenAI Embeddings
- âœ… E2E æµ‹è¯•é€šè¿‡

### Week 6 ç»“æŸ
- âœ… æ··åˆæ£€ç´¢å®ç°
- âœ… é‡æ’åºå®ç°
- âœ… å¤šå‘é‡æ•°æ®åº“æ”¯æŒ
- âœ… æ–‡æ¡£å®Œå–„

### Week 7+ (å¯é€‰)
- âœ… GraphRAG
- âœ… Agentic RAG
- âœ… æ€§èƒ½ä¼˜åŒ–

---

## ğŸ¯ æˆåŠŸæŒ‡æ ‡

**æŠ€æœ¯æŒ‡æ ‡**:
- æ£€ç´¢å‡†ç¡®ç‡: > 80%
- å“åº”æ—¶é—´: < 2 ç§’
- æ”¯æŒæ–‡æ¡£æ ¼å¼: 4+ ç§
- æ”¯æŒåµŒå…¥æ¨¡å‹: 3+ ç§
- æ”¯æŒå‘é‡æ•°æ®åº“: 3+ ç§

**ç”¨æˆ·ä½“éªŒ**:
- ä¸€é”®ç”Ÿæˆ RAG Agent
- è‡ªåŠ¨é…ç½®æœ€ä½³ç­–ç•¥
- æ¸…æ™°çš„é”™è¯¯æç¤º
- å®Œå–„çš„æ–‡æ¡£

**ä»£ç è´¨é‡**:
- æµ‹è¯•è¦†ç›–ç‡: > 70%
- ä»£ç å¯ç»´æŠ¤æ€§: é«˜
- æ¨¡å—åŒ–ç¨‹åº¦: é«˜
- æ–‡æ¡£å®Œæ•´æ€§: é«˜

---

## ğŸ“š å‚è€ƒèµ„æ–™

1. **LangChain RAG æ–‡æ¡£**
   - https://python.langchain.com/docs/use_cases/question_answering/

2. **å‘é‡æ•°æ®åº“å¯¹æ¯”**
   - ChromaDB: è½»é‡çº§,æ˜“ç”¨
   - Qdrant: é«˜æ€§èƒ½,Rust ç¼–å†™
   - Milvus: å¤§è§„æ¨¡,ä¼ä¸šçº§

3. **åµŒå…¥æ¨¡å‹é€‰æ‹©**
   - OpenAI: æ•ˆæœå¥½,æˆæœ¬é«˜
   - BGE-M3: ä¸­æ–‡å¼º,å¼€æº
   - Ollama: æœ¬åœ°éƒ¨ç½²,éšç§

4. **æœ€ä½³å®è·µ**
   - Hybrid Search å¿…é¡»ä¸Š
   - Rerank æå‡ 10-20%
   - Chunk size æ ¹æ®åœºæ™¯è°ƒæ•´

---

**å¼€å§‹å®æ–½!** ğŸš€
