"""
Phase 4 Task 4.2 æµ‹è¯• - éªŒè¯ TestGenerator

æµ‹è¯•ç›®æ ‡:
1. éªŒè¯ TestGenerator ç±»æ­£ç¡®å®ç°
2. éªŒè¯ç”Ÿæˆçš„æµ‹è¯•ä»£ç åŒ…å«å¿…è¦çš„å¯¼å…¥å’Œé…ç½®
3. éªŒè¯ RAG æµ‹è¯•ä½¿ç”¨å¤–éƒ¨ Trace
4. éªŒè¯ç®€åŒ–çš„ Ollama é›†æˆ
"""

import pytest
from pathlib import Path
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.core.test_generator import TestGenerator, DeepEvalTestConfig
from src.schemas.project_meta import ProjectMeta, TaskType
from src.schemas.rag_config import RAGConfig


def test_deepeval_test_config():
    """æµ‹è¯• 1: éªŒè¯ DeepEvalTestConfig Schema"""

    config = DeepEvalTestConfig(
        num_rag_tests=5,
        num_logic_tests=3,
        use_local_llm=True,
        judge_model="llama3",
        deepeval_version="0.21.0",
    )

    assert config.num_rag_tests == 5
    assert config.num_logic_tests == 3
    assert config.use_local_llm is True
    assert config.judge_model == "llama3"
    assert config.deepeval_version == "0.21.0"

    print("âœ… æµ‹è¯• 1 é€šè¿‡: DeepEvalTestConfig Schema æ­£ç¡®")


def test_generate_imports():
    """æµ‹è¯• 2: éªŒè¯å¯¼å…¥è¯­å¥ç”Ÿæˆ"""

    # åˆ›å»º mock LLM client
    class MockLLMClient:
        async def generate(self, prompt: str) -> str:
            return "mock response"

    generator = TestGenerator(MockLLMClient())
    config = DeepEvalTestConfig()

    imports = generator._generate_imports(config)

    # éªŒè¯åŒ…å«å¿…è¦çš„å¯¼å…¥
    assert "from deepeval import assert_test" in imports
    assert "from deepeval.test_case import LLMTestCase" in imports
    assert "from deepeval.metrics import" in imports
    assert "FaithfulnessMetric" in imports
    assert "ContextualRecallMetric" in imports
    assert "GEval" in imports
    assert "from agent import run_agent" in imports

    print("âœ… æµ‹è¯• 2 é€šè¿‡: å¯¼å…¥è¯­å¥æ­£ç¡®ç”Ÿæˆ")


def test_generate_deepeval_config_optimized():
    """æµ‹è¯• 3: éªŒè¯ç®€åŒ–çš„ Ollama é…ç½®ç”Ÿæˆ"""

    class MockLLMClient:
        async def generate(self, prompt: str) -> str:
            return "mock response"

    generator = TestGenerator(MockLLMClient())
    config = DeepEvalTestConfig(use_local_llm=True, judge_model="llama3")

    config_code = generator._generate_deepeval_config_optimized(config)

    # éªŒè¯ä½¿ç”¨ç®€åŒ–çš„ Ollama é›†æˆ
    assert "ChatOllama" in config_code
    assert "ChatOpenAI" in config_code
    assert 'model="llama3"' in config_code or 'model="{config.judge_model}"' in config_code
    assert 'base_url="http://localhost:11434"' in config_code

    # éªŒè¯ä½¿ç”¨äº† AgentZeroJudge é€‚é…å™¨ç±»
    assert "class AgentZeroJudge" in config_code
    assert "DeepEvalBaseLLM" in config_code

    # éªŒè¯æ³¨é‡Šè¯´æ˜è¿™æ˜¯ä¼˜åŒ–ç‰ˆ
    assert "ä¼˜åŒ–ç‰ˆ" in config_code or "åŠ¨æ€é€‚é…" in config_code

    print("âœ… æµ‹è¯• 3 é€šè¿‡: Ollama é…ç½®ç®€åŒ–æ­£ç¡®")


def test_generate_rag_tests_structure():
    """æµ‹è¯• 4: éªŒè¯ RAG æµ‹è¯•ä»£ç ç»“æ„"""

    class MockLLMClient:
        async def generate(self, prompt: str) -> str:
            return "mock response"

    generator = TestGenerator(MockLLMClient())

    # æ‰‹åŠ¨è°ƒç”¨å†…éƒ¨æ–¹æ³•æµ‹è¯•ç»“æ„
    qa_pairs = [
        {"question": "æµ‹è¯•é—®é¢˜1", "expected_answer": "æµ‹è¯•ç­”æ¡ˆ1"},
        {"question": "æµ‹è¯•é—®é¢˜2", "expected_answer": "æµ‹è¯•ç­”æ¡ˆ2"},
    ]

    # ç”Ÿæˆæµ‹è¯•å‡½æ•° (æ¨¡æ‹Ÿ)
    test_code = generator._generate_rag_tests.__doc__

    # éªŒè¯æ–‡æ¡£å­—ç¬¦ä¸²è¯´æ˜äº†ä½¿ç”¨å¤–éƒ¨ Trace
    assert "å¤–éƒ¨ Trace" in test_code or "å¤–éƒ¨ trace" in test_code

    print("âœ… æµ‹è¯• 4 é€šè¿‡: RAG æµ‹è¯•ç»“æ„æ­£ç¡®")


def test_parse_json_response():
    """æµ‹è¯• 5: éªŒè¯ JSON å“åº”è§£æ"""

    class MockLLMClient:
        async def generate(self, prompt: str) -> str:
            return "mock response"

    generator = TestGenerator(MockLLMClient())

    # æµ‹è¯•å¸¦ JSON ä»£ç å—çš„å“åº”
    response_with_block = """
è¿™æ˜¯ä¸€äº›è¯´æ˜æ–‡å­—

```json
[
  {"question": "Q1", "expected_answer": "A1"},
  {"question": "Q2", "expected_answer": "A2"}
]
```

æ›´å¤šè¯´æ˜
"""

    qa_pairs = generator._parse_json_response(response_with_block)
    assert len(qa_pairs) == 2
    assert qa_pairs[0]["question"] == "Q1"
    assert qa_pairs[1]["expected_answer"] == "A2"

    # æµ‹è¯•ç›´æ¥ JSON å“åº”
    response_direct = '[{"question": "Q3", "expected_answer": "A3"}]'
    qa_pairs = generator._parse_json_response(response_direct)
    assert len(qa_pairs) == 1
    assert qa_pairs[0]["question"] == "Q3"

    print("âœ… æµ‹è¯• 5 é€šè¿‡: JSON è§£ææ­£ç¡®")


def test_validate_qa_pairs():
    """æµ‹è¯• 6: éªŒè¯é—®ç­”å¯¹éªŒè¯å’Œæ¸…ç†"""

    class MockLLMClient:
        async def generate(self, prompt: str) -> str:
            return "mock response"

    generator = TestGenerator(MockLLMClient())

    # æµ‹è¯•æ­£å¸¸æƒ…å†µ
    qa_pairs = [
        {"question": "  Q1  ", "expected_answer": "  A1  "},
        {"question": "Q2", "expected_answer": "A2"},
    ]
    validated = generator._validate_qa_pairs(qa_pairs, 2)
    assert len(validated) == 2
    assert validated[0]["question"] == "Q1"  # åº”è¯¥å»é™¤ç©ºæ ¼

    # æµ‹è¯•æ•°é‡ä¸è¶³çš„æƒ…å†µ
    qa_pairs = [{"question": "Q1", "expected_answer": "A1"}]
    validated = generator._validate_qa_pairs(qa_pairs, 3)
    assert len(validated) == 3  # åº”è¯¥è¡¥å……åˆ° 3 ä¸ª

    # æµ‹è¯•æ— æ•ˆæ•°æ®
    qa_pairs = [
        {"question": "Q1", "expected_answer": "A1"},
        {"invalid": "data"},  # æ— æ•ˆ
        {"question": "Q2"},  # ç¼ºå°‘ expected_answer
    ]
    validated = generator._validate_qa_pairs(qa_pairs, 2)
    assert len(validated) == 2
    assert validated[0]["question"] == "Q1"

    print("âœ… æµ‹è¯• 6 é€šè¿‡: é—®ç­”å¯¹éªŒè¯æ­£ç¡®")


def test_heuristic_fallback():
    """æµ‹è¯• 7: éªŒè¯å¯å‘å¼å›é€€"""

    class MockLLMClient:
        async def generate(self, prompt: str) -> str:
            return "mock response"

    generator = TestGenerator(MockLLMClient())

    qa_pairs = generator._heuristic_generate_qa_pairs(3)

    assert len(qa_pairs) == 3
    assert all("question" in qa and "expected_answer" in qa for qa in qa_pairs)
    assert qa_pairs[0]["question"] == "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•é—®é¢˜ 1"

    print("âœ… æµ‹è¯• 7 é€šè¿‡: å¯å‘å¼å›é€€æ­£ç¡®")


def test_load_prompt_template():
    """æµ‹è¯• 8: éªŒè¯ Prompt æ¨¡æ¿åŠ è½½"""

    class MockLLMClient:
        async def generate(self, prompt: str) -> str:
            return "mock response"

    generator = TestGenerator(MockLLMClient())

    # æµ‹è¯•åŠ è½½ RAG æ¨¡æ¿
    template = generator._load_prompt_template("test_generator_deepeval_rag.txt")
    assert len(template) > 0
    assert "{num_tests}" in template or "{document_content}" in template

    print("âœ… æµ‹è¯• 8 é€šè¿‡: Prompt æ¨¡æ¿åŠ è½½æ­£ç¡®")


def test_parse_malformed_json_missing_braces():
    """æµ‹è¯• 9: éªŒè¯è§£æç¼ºå°‘èŠ±æ‹¬å·çš„ JSON (Tier 2)"""

    class MockLLMClient:
        async def generate(self, prompt: str) -> str:
            return "mock response"

    generator = TestGenerator(MockLLMClient())

    # æ¨¡æ‹Ÿ LLM è¿”å›ç¼ºå°‘èŠ±æ‹¬å·çš„ JSON
    malformed_response = """[
    "question": "Agent Zeroé˜¶æ®µ3æ˜¯ä»€ä¹ˆ",
    "expected_answer": "è“å›¾ä»¿çœŸç³»ç»Ÿ"
]"""

    qa_pairs = generator._parse_json_response(malformed_response)

    # åº”è¯¥èƒ½é€šè¿‡ Tier 2 æ ¼å¼ä¿®å¤è§£ææˆåŠŸ
    assert len(qa_pairs) >= 1
    if len(qa_pairs) > 0:
        assert "question" in qa_pairs[0]
        assert "expected_answer" in qa_pairs[0]

    print("âœ… æµ‹è¯• 9 é€šè¿‡: ç¼ºå°‘èŠ±æ‹¬å·çš„ JSON èƒ½è¢«ä¿®å¤")


def test_parse_json_with_regex_fallback():
    """æµ‹è¯• 10: éªŒè¯æ­£åˆ™æå–å…œåº• (Tier 3)"""

    class MockLLMClient:
        async def generate(self, prompt: str) -> str:
            return "mock response"

    generator = TestGenerator(MockLLMClient())

    # æ¨¡æ‹Ÿå®Œå…¨æ— æ•ˆçš„ JSON,ä½†åŒ…å«é—®ç­”å¯¹
    invalid_json = """
    è¿™æ˜¯ä¸€äº›è¯´æ˜æ–‡å­—
    "question": "ä»€ä¹ˆæ˜¯ RAG?"
    "expected_answer": "æ£€ç´¢å¢å¼ºç”Ÿæˆ"
    è¿˜æœ‰ä¸€äº›å…¶ä»–æ–‡å­—
    "question": "ä»€ä¹ˆæ˜¯ LangGraph?"
    "expected_answer": "çŠ¶æ€å›¾æ‰§è¡Œå¼•æ“"
    """

    qa_pairs = generator._parse_json_response(invalid_json)

    # åº”è¯¥èƒ½é€šè¿‡ Tier 3 æ­£åˆ™æå–
    assert len(qa_pairs) == 2
    assert qa_pairs[0]["question"] == "ä»€ä¹ˆæ˜¯ RAG?"
    assert qa_pairs[1]["expected_answer"] == "çŠ¶æ€å›¾æ‰§è¡Œå¼•æ“"

    print("âœ… æµ‹è¯• 10 é€šè¿‡: æ­£åˆ™æå–å…œåº•æˆåŠŸ")


def test_parse_mixed_valid_invalid():
    """æµ‹è¯• 11: éªŒè¯å¤„ç†æ··åˆæœ‰æ•ˆ/æ— æ•ˆçš„å“åº”"""

    class MockLLMClient:
        async def generate(self, prompt: str) -> str:
            return "mock response"

    generator = TestGenerator(MockLLMClient())

    # æµ‹è¯•éƒ¨åˆ†æœ‰æ•ˆçš„ JSON
    mixed_response = """```json
[
  {
    "question": "æœ‰æ•ˆé—®é¢˜1",
    "expected_answer": "æœ‰æ•ˆç­”æ¡ˆ1"
  },
  "question": "ç¼ºå°‘èŠ±æ‹¬å·",
  "expected_answer": "è¿™ä¸ªä¼šè¢«ä¿®å¤"
]
```"""

    qa_pairs = generator._parse_json_response(mixed_response)

    # åº”è¯¥èƒ½è§£æå‡ºè‡³å°‘ä¸€ä¸ª
    assert len(qa_pairs) >= 1

    print("âœ… æµ‹è¯• 11 é€šè¿‡: æ··åˆæœ‰æ•ˆ/æ— æ•ˆå“åº”å¤„ç†æ­£ç¡®")


if __name__ == "__main__":
    print("=" * 60)
    print("Phase 4 Task 4.2 æµ‹è¯• - TestGenerator (å¢å¼ºç‰ˆ)")
    print("=" * 60)

    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    test_deepeval_test_config()
    test_generate_imports()
    test_generate_deepeval_config_optimized()
    test_generate_rag_tests_structure()
    test_parse_json_response()
    test_validate_qa_pairs()
    test_heuristic_fallback()
    test_load_prompt_template()

    # æ–°å¢: JSON è§£æå¢å¼ºæµ‹è¯•
    print("\n" + "-" * 60)
    print("ğŸ†• JSON è§£æå¢å¼ºæµ‹è¯•")
    print("-" * 60)
    test_parse_malformed_json_missing_braces()
    test_parse_json_with_regex_fallback()
    test_parse_mixed_valid_invalid()

    print("\n" + "=" * 60)
    print("âœ… æ‰€æœ‰ 11 ä¸ªæµ‹è¯•é€šè¿‡! JSON è§£æå¢å¼ºå®Œæˆ!")
    print("=" * 60)
